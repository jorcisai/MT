{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase-based models\n",
    "\n",
    "\n",
    "Motivation: Capture many-to-many mapping to include word context\n",
    "\n",
    "State-of-the-art MT models until about 2017 (Google Translate)\n",
    "\n",
    "Generative phrase-based models [1,2]: hidden variables for phrase segmentation and alignment\n",
    "\n",
    "Heuristic phrase-based models: alignment symmetrisation and relative counts [3, Chapter 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment symmetrisation and phrase extraction\n",
    "\n",
    "\n",
    "Learn word-based alignment model from target to source $a$ and from source to target $b$\n",
    "\n",
    "Take the intersection $a \\cap b$ and heuristically include alignments from $a \\cup b$\n",
    "\n",
    "Extract all possible phrases *consistent* with the alignment previously selected\n",
    "\n",
    "Consistent means that all words of the phrase pair $(\\bar{x},\\bar{y})$ are aligned to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import AlignedSent, IBMModel2, phrase_based\n",
    "\n",
    "esText = ['la casa es azul','mi casa era blanca','mi perro es blanco','el perro era azul']\n",
    "enText = ['the house is blue', 'my house was white','my dog is white', 'the dog was blue']\n",
    "\n",
    "# Source language is Euskera and target language is English\n",
    "corpus = []\n",
    "for enSent, esSent in zip(enText,esText):\n",
    "    corpus.append(AlignedSent(enSent.split(),esSent.split()))\n",
    "\n",
    "m2 = IBMModel2(corpus, 5)\n",
    "m2.align_all(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the house is blue > la casa es azul: 0-3 1-1 2-2 3-3\n",
      "((0, 4), (0, 4), 'the house is blue', 'la casa es azul')\n",
      "((0, 4), (1, 4), 'the house is blue', 'casa es azul')\n",
      "((1, 2), (0, 2), 'house', 'la casa')\n",
      "((1, 2), (1, 2), 'house', 'casa')\n",
      "((1, 3), (0, 3), 'house is', 'la casa es')\n",
      "((1, 3), (1, 3), 'house is', 'casa es')\n",
      "((2, 3), (2, 3), 'is', 'es')\n",
      "my house was white > mi casa era blanca: 0-3 1-1 2-2 3-3\n",
      "((0, 4), (0, 4), 'my house was white', 'mi casa era blanca')\n",
      "((0, 4), (1, 4), 'my house was white', 'casa era blanca')\n",
      "((1, 2), (0, 2), 'house', 'mi casa')\n",
      "((1, 2), (1, 2), 'house', 'casa')\n",
      "((1, 3), (0, 3), 'house was', 'mi casa era')\n",
      "((1, 3), (1, 3), 'house was', 'casa era')\n",
      "((2, 3), (2, 3), 'was', 'era')\n",
      "my dog is white > mi perro es blanco: 0-3 1-1 2-2 3-3\n",
      "((0, 4), (0, 4), 'my dog is white', 'mi perro es blanco')\n",
      "((0, 4), (1, 4), 'my dog is white', 'perro es blanco')\n",
      "((1, 2), (0, 2), 'dog', 'mi perro')\n",
      "((1, 2), (1, 2), 'dog', 'perro')\n",
      "((1, 3), (0, 3), 'dog is', 'mi perro es')\n",
      "((1, 3), (1, 3), 'dog is', 'perro es')\n",
      "((2, 3), (2, 3), 'is', 'es')\n",
      "the dog was blue > el perro era azul: 0-3 1-1 2-2 3-3\n",
      "((0, 4), (0, 4), 'the dog was blue', 'el perro era azul')\n",
      "((0, 4), (1, 4), 'the dog was blue', 'perro era azul')\n",
      "((1, 2), (0, 2), 'dog', 'el perro')\n",
      "((1, 2), (1, 2), 'dog', 'perro')\n",
      "((1, 3), (0, 3), 'dog was', 'el perro era')\n",
      "((1, 3), (1, 3), 'dog was', 'perro era')\n",
      "((2, 3), (2, 3), 'was', 'era')\n"
     ]
    }
   ],
   "source": [
    "for biSent in corpus:\n",
    "    esSent = \" \".join(biSent.words)\n",
    "    enSent = \" \".join(biSent.mots)\n",
    "    print(f'{esSent} > {enSent}: {biSent.alignment}')\n",
    "    phrases = phrase_based.phrase_extraction(esSent,enSent, biSent.alignment)\n",
    "    for i in sorted(phrases):\n",
    "         print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model estimation\n",
    "\n",
    "$p(\\bar{y} \\mid \\bar{x})$ are estimated as relative counts\n",
    "\n",
    "In practice, phrase-based systems involved not only phrase-based models, but additional models:\n",
    "\n",
    "- Phrase-based models in both directions\n",
    "\n",
    "- Word-based models in both directions\n",
    "\n",
    "- Target language model\n",
    "\n",
    "- Reordering model\n",
    "\n",
    "- Phrase penalty: log of number of phrases involved\n",
    "\n",
    "- etc.\n",
    "\n",
    "combined in a log-linear fashion:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y} &= \\argmax_{y} P(y \\mid x)\\\\% \n",
    "        &= \\argmax_{y} \\sum_m \\lambda_m h_m (x, y)\n",
    "\\end{align*}        \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "A* algorithm based on dynamic programming, multi-stack decoding and hypothesis pruning\n",
    "\n",
    "Incremental development of partial hypothesis\n",
    "\n",
    "Each hypothesis is characterised by a coverage of the source sentence, a target prefix and a score\n",
    "\n",
    "Each stack stores hypotheses with the same coverage of the source sentence sorted by score from highest to lowest\n",
    "\n",
    "Hypotheses from the top of the stacks are selected for expansion\n",
    "\n",
    "Hypothesis prunning based on:\n",
    "\n",
    "- Beam-search (relative difference w.r.t. best scoring hypothesis) or threshold \n",
    "\n",
    "- Histogram prunning (max. number of hypotheses per stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ol>\n",
    "<li><a href=\"https://aclanthology.org/W02-1018.pdf\" target=\"_blank\">D. Marcu and W. Wong. A Phrase-Based, Joint Probability Model for Statistical Machine Translation, EMNLP 2002.</a></li>\n",
    "<li><a href=\"https://aclanthology.org/2009.eamt-1.23.pdf\" target=\"_blank\">J. Andr√©s-Ferrer and A. Juan. A Phrase-Based Hidden Semi-Markov Approach to Machine Translation, EAMT 2009.</a></li>\n",
    "<li><a href=\"https://doi.org/10.1017/CBO9780511815829\" target=\"_blank\">P. Koehn. Statistical Machine Translation, MIT Press 2010.</a></li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
