{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from data\n",
    "\n",
    "<ul>\n",
    "<li><b>Monolingual data</b></li>\n",
    "    Ex.: Mary did not slap the green witch.\n",
    "<li><b>Multilingual data</b></li>\n",
    "    Ex.: Mary did not slap the green witch. Mary no dió una botefada a la bruja verde.\n",
    "<li><b>Parallel data</b></li>\n",
    "<ul>\n",
    "<li><b>Text-To-Text.</b></li>\n",
    "    Ex.: Mary did not slap the green witch. <b>||</b> Mary no dió una botefada a la bruja verde.\n",
    "<li><b>Speech-To-Text.</b> Automatic speech recognition or speech translation</li> \n",
    "<li><b>Text-To-Speech.</b> Speech synthesis</li>\n",
    "<li><b>Speech-To-Speech</b></li>\n",
    "</ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from parallel data: text-to-text\n",
    "\n",
    "Example of parallel text:\n",
    "<table>\n",
    "<tr><td>my house is blue</td><td>nire etxea urdina da</td></tr>\n",
    "<tr><td>my house is white</td><td>nire etxea zuria da</td></tr>\n",
    "<tr><td>my dog was white</td><td>nire txakurra zuria zen</td></tr>\n",
    "<tr><td>the dog was blue</td><td>txakurra urdina zen</td></tr>\n",
    "</table>\n",
    "\n",
    "Exercise: Can you identify which words are mutual translations? That is, define a bilingual dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "<table>\n",
    "<tr><td>my</td><td>nire</td></tr>\n",
    "<tr><td>house</td><td>etxea</td></tr>\n",
    "<tr><td>is</td><td>da</td></tr>\n",
    "<tr><td>blue</td><td>urdina</td></tr>\n",
    "<tr><td>dog</td><td>txakurra</td></tr>\n",
    "<tr><td>was</td><td>zen</td></tr>\n",
    "<tr><td>the</td><td>NULL</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>The concept of <b>alignment</b> between source and target words naturally arises.</li>\n",
    "<li>If alignments were available, it would be straightforward to derive a bilingual dictionary.</li>\n",
    "<li>Can we automatically learn word alignments from parallel text?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-based alignment models\n",
    "\n",
    "\n",
    "Let $x = x_1 \\cdots x_{|x|} = x_1^{|x|}$ and $y = y_1 \\cdots y_{|y|} = y_1^{|y|}$ be source and target sentences that are mutual translations. The variables $x_j$ and $y_i$ denote the $j$-th source word and the $i$-th target word, respectively. For the sake of clarity, let $J=|x|$ and $I=|y|$ be the number of source and target words, respectively.\n",
    "\n",
    "Let $a = a_1 \\cdots a_J$ be an alignment variable that assigns each target position to a source position. That is, $a_j \\in \\{1,\\cdots,I\\}$. For example, in the first sentence above, $a=(1, 2, 4, 3)$.\n",
    "\n",
    "More precisely, a ficticius target position $i=0$ is defined to account for those positions in the source sentence that are not aligned to any target position. Thus, $a_i \\in \\{0, 1,\\cdots,I\\}$. So, the last sentence would be $a=(0, 2, 4, 3)$.\n",
    "\n",
    "The alignment is considered a hidden variable, so that we sum over all its possible values:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(x \\mid y) &= \\sum_a P(x, a \\mid y)\\\\%\n",
    "            &= \\sum_a \\prod_j P(x_j, a_j \\mid x, x_1^{j-1}, a_1^{j-1}, x)\\\\%\n",
    "            &= \\sum_a \\prod_j P(x_j \\mid y, x_1^{j-1}, a_1^{j}, x) \\, P(a_j \\mid x, y_1^{j-1}, a_1^{j-1}, x)%\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Model 1\n",
    "\n",
    "Assumptions and model parameters:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(x_j \\mid y, x_1^{j-1}, a_1^{j}, x)   &:= p(x_j \\mid y_{a_j})\\\\ \n",
    "P(a_j \\mid y, x_1^{j-1}, a_1^{j-1}, x) &:= \\frac{1}{I+1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Model 1 is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(x \\mid y) &\\approx \\sum_a \\prod_j \\frac{1}{I+1} \\, p(x_j \\mid y_{a_j})\\\\%\n",
    "            &=       \\prod_j \\sum_{a_j} \\frac{1}{I+1} \\, p(x_j \\mid y_{a_j})\\\\%\n",
    "            &= \\frac{1}{(I+1)^J} \\, \\prod_j \\sum_{a_j} p(x_j \\mid y_{a_j})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Parameter optimization of log-likelihood by EM algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{E step}: a_{nji} &= \\frac{p(x_{nj} \\mid y_{ni})}{\\sum_{i'} p(x_{nj} \\mid y_{ni'})}\\\\%\n",
    "\\text{M step}: p(u \\mid v) &\\sim  \\sum_n \\sum_{j:x_{nj}=u} \\sum_{i:y_{ni}=v} a_{nji}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other word-based models\n",
    "\n",
    "<ul>\n",
    "<li>IBM research group proposed models 1 through 5</li>\n",
    "<li>HMM alignment model</li>\n",
    "<li>Mixture models</li>\n",
    "<li>etc.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"https://kevincrawfordknight.github.io/papers/wkbk-rw.pdf\" target=\"_blank\">K. Knight. A Statistical MT Tutorial Workbook, August 1999.</a></li>\n",
    "<li><a href=\"https://github.com/moses-smt/giza-pp\" target=\"_blank\">F. Och. GIZA++ toolkit and the mkcls tool.</a></li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
