{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from data\n",
    "\n",
    "<ul>\n",
    "<li><b>Monolingual data</b></li>\n",
    "    Ex.: Mary did not slap the green witch.\n",
    "<li><b>Multilingual data</b></li>\n",
    "    Ex.: Mary did not slap the green witch. Mary no dió una botefada a la bruja verde.\n",
    "<li><b>Parallel data</b></li>\n",
    "<ul>\n",
    "<li><b>Text-To-Text.</b></li>\n",
    "    Ex.: Mary did not slap the green witch. <b>||</b> Mary no dió una botefada a la bruja verde.\n",
    "<li><b>Speech-To-Text.</b> Automatic speech recognition or speech translation</li> \n",
    "<li><b>Text-To-Speech.</b> Speech synthesis</li>\n",
    "<li><b>Speech-To-Speech</b></li>\n",
    "</ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from parallel data: text-to-text\n",
    "\n",
    "Example of parallel text:\n",
    "<table>\n",
    "<tr><td>my house is blue</td><td>nire etxea urdina da</td></tr>\n",
    "<tr><td>my house is white</td><td>nire etxea zuria da</td></tr>\n",
    "<tr><td>my dog was white</td><td>nire txakurra zuria zen</td></tr>\n",
    "<tr><td>the dog was blue</td><td>txakurra urdina zen</td></tr>\n",
    "</table>\n",
    "\n",
    "Exercise: Can you identify which words are mutual translations? That is, define a bilingual dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "<table>\n",
    "<tr><td>my</td><td>nire</td></tr>\n",
    "<tr><td>house</td><td>etxea</td></tr>\n",
    "<tr><td>is</td><td>da</td></tr>\n",
    "<tr><td>blue</td><td>urdina</td></tr>\n",
    "<tr><td>dog</td><td>txakurra</td></tr>\n",
    "<tr><td>was</td><td>zen</td></tr>\n",
    "<tr><td>the</td><td>NULL</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>The concept of <b>alignment</b> between source and target words naturally arises.</li>\n",
    "<li>If alignments were available, it would be straightforward to derive a bilingual dictionary.</li>\n",
    "<li>Can we automatically learn word alignments from parallel text?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-based alignment models\n",
    "\n",
    "\n",
    "Let $x = x_1 \\cdots x_{|x|} = x_1^{|x|}$ and $y = y_1 \\cdots y_{|y|} = y_1^{|y|}$ be source and target sentences that are mutual translations. The variables $x_j$ and $y_i$ denote the $j$-th source word and the $i$-th target word, respectively. For the sake of clarity, let $J=|x|$ and $I=|y|$ be the number of source and target words, respectively.\n",
    "\n",
    "Let $a = a_1 \\cdots a_J$ be an alignment variable that assigns each target position to a source position. That is, $a_j \\in \\{1,\\cdots,I\\}$. For example, in the first sentence above, $a=(1, 2, 4, 3)$.\n",
    "\n",
    "More precisely, a ficticius target position $i=0$ (NULL word) is defined to account for those positions in the source sentence that are not aligned to any target position. Thus, $a_i \\in \\{0, 1,\\cdots,I\\}$. So, the last sentence would be $a=(0, 2, 4, 3)$.\n",
    "\n",
    "The alignment is considered a hidden variable, so that we sum over all its possible values:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(x \\mid y) &= \\sum_a P(x, a \\mid y)\\\\%\n",
    "            &= \\sum_a \\prod_j P(x_j, a_j \\mid x, x_1^{j-1}, a_1^{j-1}, x)\\\\%\n",
    "            &= \\sum_a \\prod_j P(x_j \\mid y, x_1^{j-1}, a_1^{j}, x) \\, P(a_j \\mid x, y_1^{j-1}, a_1^{j-1}, x)%\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Model 1\n",
    "\n",
    "Assumptions and model parameters:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(x_j \\mid y, x_1^{j-1}, a_1^{j}, x)   &:= p(x_j \\mid y_{a_j})\\\\ \n",
    "P(a_j \\mid y, x_1^{j-1}, a_1^{j-1}, x) &:= \\frac{1}{I+1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Model 1 is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(x \\mid y) &\\approx \\sum_a \\prod_j \\frac{1}{I+1} \\, p(x_j \\mid y_{a_j})\\\\%\n",
    "            &=       \\prod_j \\sum_{a_j} \\frac{1}{I+1} \\, p(x_j \\mid y_{a_j})\\\\%\n",
    "            &= \\frac{1}{(I+1)^J} \\, \\prod_j \\sum_{a_j} p(x_j \\mid y_{a_j})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Parameter optimization of log-likelihood by EM algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{E step}: a_{nji} &= \\frac{p(x_{nj} \\mid y_{ni})}{\\sum_{i'} p(x_{nj} \\mid y_{ni'})}\\\\%\n",
    "\\text{M step}: p(u \\mid v) &\\sim  \\sum_n \\sum_{j:x_{nj}=u} \\sum_{i:y_{ni}=v} a_{nji}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srcData = [[1, 2, 3, 4], [1, 2, 3, 5], [1, 6, 7, 5], [8, 6, 7, 4]]\n",
      "trgData = [[1, 2, 3, 4], [1, 2, 5, 4], [1, 6, 5, 7], [6, 3, 7]]\n",
      "M1Dict = [[0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]]\n",
      "M1Dict = [[0.2   0.077 0.077 0.198 0.11  0.146 0.146 0.046]\n",
      " [0.288 0.111 0.111 0.135 0.158 0.099 0.099 0.   ]\n",
      " [0.333 0.203 0.203 0.117 0.143 0.    0.    0.   ]\n",
      " [0.142 0.133 0.133 0.195 0.    0.147 0.147 0.103]\n",
      " [0.203 0.236 0.236 0.159 0.167 0.    0.    0.   ]\n",
      " [0.237 0.142 0.142 0.    0.198 0.141 0.141 0.   ]\n",
      " [0.181 0.    0.    0.178 0.138 0.212 0.212 0.079]\n",
      " [0.128 0.    0.    0.151 0.147 0.212 0.212 0.152]]\n",
      "NULL -> my\n",
      "nire -> my\n",
      "etxea -> my\n",
      "urdina -> blue\n",
      "da -> house\n",
      "zuria -> my\n",
      "txakurra -> dog\n",
      "zen -> dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "def create_dataset(sents):\n",
    "    dict = {}\n",
    "    idict = {}\n",
    "    idSents = []\n",
    "    id = 1\n",
    "    for sent in sents:\n",
    "        idSent = []\n",
    "        for word in sent.split():\n",
    "            if word not in dict:\n",
    "                dict[word] = id\n",
    "                idict[id] = word\n",
    "                idSent.append(id)\n",
    "                id += 1                 \n",
    "            else:\n",
    "                idSent.append(dict[word])\n",
    "        idSents.append(idSent)\n",
    "    return dict, idict, idSents\n",
    "    \n",
    "srcSents = ['my house is blue','my house is white','my dog was white','the dog was blue']\n",
    "trgSents = ['nire etxea urdina da','nire etxea zuria da','nire txakurra zuria zen','txakurra urdina zen']\n",
    "\n",
    "srcDict, isrcDict, srcData = create_dataset(srcSents)\n",
    "trgDict, itrgDict, trgData = create_dataset(trgSents)\n",
    "\n",
    "print(f'srcData = {srcData}')\n",
    "print(f'trgData = {trgData}')\n",
    "\n",
    "# M1 dictionary initialise with uniform distro\n",
    "M1Dict = np.zeros((len(trgDict)+1,len(srcDict)),dtype=float)\n",
    "for trgWord in range(len(M1Dict)):\n",
    "    M1Dict[trgWord] = 1.0/len(srcDict)\n",
    "print(f'M1Dict = {M1Dict}')\n",
    "\n",
    "\n",
    "for iter in range(10):\n",
    "    newM1Dict = np.zeros((len(trgDict)+1,len(srcDict)),dtype=float)\n",
    "    for n in range(len(srcData)): \n",
    "        # E-step\n",
    "        a = np.zeros((len(srcData[n]), len(trgData[n])+1),dtype=float)\n",
    "        for j in range(len(srcData[n])):\n",
    "            # NULL word\n",
    "            a[j][0] = M1Dict[0][srcData[n][j]-1]\n",
    "            suma = a[j][0]\n",
    "            for i in range(len(trgData[n])):\n",
    "                a[j][i+1] = M1Dict[trgData[n][i]][srcData[n][j]-1]\n",
    "                suma += a[j][i+1]\n",
    "            a[j][0] /= suma\n",
    "            for i in range(len(trgData[n])):\n",
    "                a[j][i+1] /= suma\n",
    "        #print(f'a =\\n{a}')\n",
    "        # M-step\n",
    "        for j in range(len(srcData[n])):\n",
    "            newM1Dict[0][srcData[n][j]-1] += a[j][0]\n",
    "            for i in range(len(trgData[n])):\n",
    "                newM1Dict[trgData[n][i]][srcData[n][j]-1] += a[j][i]\n",
    "        #print(f'newM1Dict = {newM1Dict}')\n",
    "\n",
    "    # Normalise to obtain probabilities\n",
    "    for trgWord in range(len(M1Dict)):\n",
    "        suma = np.sum(newM1Dict[trgWord])\n",
    "        for srcWord in range(len(M1Dict[trgWord])):\n",
    "            newM1Dict[trgWord][srcWord] /= suma\n",
    "\n",
    "    # Update M1 dictionary\n",
    "    M1Dict = newM1Dict\n",
    "print(f'M1Dict = {M1Dict}')\n",
    "\n",
    "print(f'NULL -> {isrcDict[np.argmax(M1Dict[0])+1]}')\n",
    "for trgWord in range(1,len(M1Dict)):\n",
    "    print(f'{itrgDict[trgWord]} -> {isrcDict[np.argmax(M1Dict[trgWord])+1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other word-based models\n",
    "\n",
    "<ul>\n",
    "<li>IBM research group proposed models 1 through 5</li>\n",
    "<li>HMM alignment model</li>\n",
    "<li>Mixture models</li>\n",
    "<li>etc.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"https://kevincrawfordknight.github.io/papers/wkbk-rw.pdf\" target=\"_blank\">K. Knight. A Statistical MT Tutorial Workbook, August 1999.</a></li>\n",
    "<li><a href=\"https://github.com/moses-smt/giza-pp\" target=\"_blank\">F. Och. GIZA++ toolkit and the mkcls tool.</a></li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
