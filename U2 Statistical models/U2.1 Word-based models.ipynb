{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from data\n",
    "\n",
    "<ul>\n",
    "<li><b>Monolingual data</b></li>\n",
    "    Ex.: Mary did not slap the green witch.\n",
    "<li><b>Multilingual data</b></li>\n",
    "    Ex.: Mary did not slap the green witch. Mary no dió una botefada a la bruja verde.\n",
    "<li><b>Parallel data</b></li>\n",
    "<ul>\n",
    "<li><b>Text-To-Text.</b></li>\n",
    "    Ex.: Mary did not slap the green witch. <b>||</b> Mary no dió una botefada a la bruja verde.\n",
    "<li><b>Speech-To-Text.</b> Automatic speech recognition or speech translation</li> \n",
    "<li><b>Text-To-Speech.</b> Speech synthesis</li>\n",
    "<li><b>Speech-To-Speech</b></li>\n",
    "</ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from parallel data: text-to-text\n",
    "\n",
    "Example of parallel text:\n",
    "<table>\n",
    "<tr><td>the house is blue</td><td>etxea urdina da</td></tr>\n",
    "<tr><td>my house was white</td><td>nire etxea zuria zen</td></tr>\n",
    "<tr><td>my dog is white</td><td>nire txakurra zuria da</td></tr>\n",
    "<tr><td>the dog was blue</td><td>txakurra urdina zen</td></tr>\n",
    "</table>\n",
    "\n",
    "Exercise: Can you identify which words are mutual translations? That is, define a bilingual dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "<table>\n",
    "<tr><td>my</td><td>nire</td></tr>\n",
    "<tr><td>house</td><td>etxea</td></tr>\n",
    "<tr><td>is</td><td>da</td></tr>\n",
    "<tr><td>blue</td><td>urdina</td></tr>\n",
    "<tr><td>dog</td><td>txakurra</td></tr>\n",
    "<tr><td>was</td><td>zen</td></tr>\n",
    "<tr><td>the</td><td>NULL</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>The concept of <b>alignment</b> between source and target words naturally arises.</li>\n",
    "<li>If alignments were available, it would be straightforward to derive a bilingual dictionary.</li>\n",
    "<li>Can we automatically learn word alignments from parallel text?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-based alignment models\n",
    "\n",
    "\n",
    "Let $x = x_1 \\cdots x_{|x|} = x_1^{|x|}$ and $y = y_1 \\cdots y_{|y|} = y_1^{|y|}$ be source and target sentences that are mutual translations. The variables $x_j$ and $y_i$ denote the $j$-th source word and the $i$-th target word, respectively. For the sake of clarity, let $J=|x|$ and $I=|y|$ be the number of source and target words, respectively.\n",
    "\n",
    "Let $a = a_1 \\cdots a_J$ be an alignment variable that assigns each target position to a source position. That is, $a_j \\in \\{1,\\cdots,I\\}$. For example, in the first sentence above, $a=(1, 2, 4, 3)$.\n",
    "\n",
    "More precisely, a ficticius target position $i=0$ (NULL word) is defined to account for those positions in the source sentence that are not aligned to any target position. Thus, $a_i \\in \\{0, 1,\\cdots,I\\}$. So, the last sentence would be $a=(0, 2, 4, 3)$.\n",
    "\n",
    "The alignment is considered a hidden variable, so that we sum over all its possible values:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(y \\mid x) &= P(y, I \\mid x)\\\\%\n",
    "            &= P(I \\mid x) \\, P(y \\mid I, x)\\\\\n",
    "            &= P(I \\mid x) \\sum_a P(y, a \\mid I, x)\\\\%\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(y, a \\mid I, x) &= \\prod_{i=1}^I P(y_i, a_i \\mid x, y_1^{i-1}, a_1^{i-1})\\\\%\n",
    "                  &= \\prod_{i=1}^I P(y_i \\mid x, y_1^{i-1}, a_1^{i}) \\, P(a_i \\mid x, y_1^{i-1}, a_1^{i-1})%\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "### Model 1\n",
    "\n",
    "Assumptions and model parameters:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(y_i \\mid x, y_1^{i-1}, a_1^{i})   &:= p(y_i \\mid x_{a_i})\\\\ \n",
    "P(a_i \\mid x, y_1^{i-1}, a_1^{i-1}) &:= \\frac{1}{J+1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Model 1 is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(x \\mid y) &\\sim \\sum_a \\prod_{i=1}^I \\frac{1}{J+1} \\, p(y_i \\mid x_{a_i})\\\\%\n",
    "            &=       \\prod_{i=1}^I \\sum_{a_i=0}^J \\frac{1}{J+1} \\, p(y_i \\mid x_{a_i})\\\\%\n",
    "            &= \\frac{1}{(J+1)^I} \\, \\prod_{i=1}^I \\sum_{a_i=0}^J p(y_i \\mid x_{a_i})\\\\%\n",
    "            &= \\frac{1}{(J+1)^I} \\, \\prod_{i=1}^I \\sum_{j=0}^J p(y_i \\mid x_j)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Parameter optimization of log-likelihood by EM algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{E step}: a_{nij} &= \\frac{p(y_{ni} \\mid x_{nj})}{\\sum_{j'} p(y_{ni} \\mid x_{nj'})}\\\\%\n",
    "\\text{M step}: p(u \\mid v) &\\sim  \\sum_n \\sum_{i:y_{ni}=u} \\sum_{j:x_{nj}=v} a_{nij}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['etxea', 'urdina', 'da'] > ['the', 'house', 'is', 'blue']\n",
      "['nire', 'etxea', 'zuria', 'zen'] > ['my', 'house', 'was', 'white']\n",
      "['nire', 'txakurra', 'zuria', 'da'] > ['my', 'dog', 'is', 'white']\n",
      "['txakurra', 'urdina', 'zen'] > ['the', 'dog', 'was', 'blue']\n",
      "[AlignedSent(['the', 'house', 'is', 'blue'], ['etxea', 'urdina', 'da'], Alignment([(0, 1), (1, 0), (2, 2), (3, 1)])), AlignedSent(['my', 'house', 'was', 'white'], ['nire', 'etxea', 'zuria', 'zen'], Alignment([(0, 2), (1, 1), (2, 3), (3, 2)])), AlignedSent(['my', 'dog', 'is', 'white'], ['nire', 'txakurra', 'zuria', 'da'], Alignment([(0, 2), (1, 1), (2, 3), (3, 2)])), AlignedSent(['the', 'dog', 'was', 'blue'], ['txakurra', 'urdina', 'zen'], Alignment([(0, 1), (1, 0), (2, 2), (3, 1)]))]\n",
      "p(was | None) = 0.11476098738559218\n",
      "p(was | nire) = 0.019614016975134548\n",
      "p(was | etxea) = 0.019819949432785266\n",
      "p(was | zuria) = 0.019614016975134548\n",
      "p(was | zen) = 0.8256223171010132\n",
      "p(was | txakurra) = 0.03293937092327597\n",
      "p(was | urdina) = 0.010651973572395548\n",
      "p(the | None) = 0.20575856359208614\n",
      "p(the | etxea) = 0.04625887733778823\n",
      "p(the | urdina) = 0.47869605285520894\n",
      "p(the | da) = 0.046258877337788225\n",
      "p(the | txakurra) = 0.04625887733778823\n",
      "p(the | zen) = 0.04625887733778823\n",
      "p(blue | None) = 0.20575856359208614\n",
      "p(blue | etxea) = 0.04625887733778823\n",
      "p(blue | urdina) = 0.47869605285520894\n",
      "p(blue | da) = 0.046258877337788225\n",
      "p(blue | txakurra) = 0.04625887733778823\n",
      "p(blue | zen) = 0.04625887733778823\n",
      "p(house | None) = 0.11476098738559223\n",
      "p(house | etxea) = 0.8256223171010132\n",
      "p(house | urdina) = 0.01065197357239555\n",
      "p(house | da) = 0.032939370923275965\n",
      "p(house | nire) = 0.019614016975134558\n",
      "p(house | zuria) = 0.019614016975134558\n",
      "p(house | zen) = 0.01981994943278528\n",
      "p(dog | None) = 0.11476098738559218\n",
      "p(dog | nire) = 0.019614016975134548\n",
      "p(dog | txakurra) = 0.8256223171010132\n",
      "p(dog | zuria) = 0.019614016975134548\n",
      "p(dog | da) = 0.019819949432785266\n",
      "p(dog | urdina) = 0.010651973572395548\n",
      "p(dog | zen) = 0.03293937092327597\n",
      "p(white | None) = 0.06471946163672945\n",
      "p(white | nire) = 0.4607719660497309\n",
      "p(white | etxea) = 0.01455030393367456\n",
      "p(white | zuria) = 0.4607719660497309\n",
      "p(white | zen) = 0.014550303933674565\n",
      "p(white | txakurra) = 0.014550303933674567\n",
      "p(white | da) = 0.014550303933674562\n",
      "p(is | None) = 0.1147609873855922\n",
      "p(is | etxea) = 0.032939370923275965\n",
      "p(is | urdina) = 0.01065197357239555\n",
      "p(is | da) = 0.8256223171010131\n",
      "p(is | nire) = 0.01961401697513455\n",
      "p(is | txakurra) = 0.019819949432785273\n",
      "p(is | zuria) = 0.01961401697513455\n",
      "p(my | None) = 0.06471946163672945\n",
      "p(my | nire) = 0.4607719660497309\n",
      "p(my | etxea) = 0.01455030393367456\n",
      "p(my | zuria) = 0.4607719660497309\n",
      "p(my | zen) = 0.014550303933674565\n",
      "p(my | txakurra) = 0.014550303933674567\n",
      "p(my | da) = 0.014550303933674562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ns={}\\nfor trgWord in m1.translation_table:\\n    for srcWord in m1.translation_table[trgWord]:\\n        if srcWord not in s:\\n            s[srcWord]  = m1.translation_table[trgWord][srcWord]\\n        else:\\n            s[srcWord] += m1.translation_table[trgWord][srcWord]\\n\\nfor srcWord in s:\\n    print(f's[{srcWord}] = {s[srcWord]}')\\n\""
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate import AlignedSent, IBMModel1\n",
    "\n",
    "enText = ['the house is blue', 'my house was white','my dog is white', 'the dog was blue']\n",
    "euText = ['etxea urdina da','nire etxea zuria zen','nire txakurra zuria da','txakurra urdina zen']\n",
    "\n",
    "# Source language is Euskera and target language is English\n",
    "corpus = []\n",
    "for enSent, euSent in zip(enText,euText):\n",
    "    corpus.append(AlignedSent(enSent.split(),euSent.split()))\n",
    "\n",
    "for sentencePair in corpus: print(f'{sentencePair.mots} > {sentencePair.words}')\n",
    "\n",
    "# Training p(trg_word | src_word): m1.translation_table[trgWord][srcWord]\n",
    "m1 = IBMModel1(corpus, 5)\n",
    "\n",
    "m1.align_all(corpus)\n",
    "\n",
    "print(corpus)\n",
    "\n",
    "for trgWord in m1.translation_table:\n",
    "    for srcWord in m1.translation_table[trgWord]:\n",
    "        print(f'p({trgWord} | {srcWord}) = {m1.translation_table[trgWord][srcWord]}')\n",
    "\n",
    "\"\"\"\n",
    "s={}\n",
    "for trgWord in m1.translation_table:\n",
    "    for srcWord in m1.translation_table[trgWord]:\n",
    "        if srcWord not in s:\n",
    "            s[srcWord]  = m1.translation_table[trgWord][srcWord]\n",
    "        else:\n",
    "            s[srcWord] += m1.translation_table[trgWord][srcWord]\n",
    "\n",
    "for srcWord in s:\n",
    "    print(f's[{srcWord}] = {s[srcWord]}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srcData = [[1, 2, 3, 4], [5, 2, 6, 7], [5, 8, 3, 7], [1, 8, 6, 4]]\n",
      "trgData = [[1, 2, 3], [4, 1, 5, 6], [4, 7, 5, 3], [7, 2, 6]]\n",
      "M1Dict = [[0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]]\n",
      "M1Dict = [[0.164 0.116 0.162 0.164 0.058 0.162 0.058 0.116]\n",
      " [0.132 0.207 0.146 0.132 0.123 0.136 0.123 0.   ]\n",
      " [0.207 0.167 0.125 0.207 0.    0.125 0.    0.167]\n",
      " [0.152 0.13  0.168 0.152 0.144 0.    0.144 0.11 ]\n",
      " [0.    0.146 0.195 0.    0.159 0.195 0.159 0.146]\n",
      " [0.    0.17  0.11  0.    0.22  0.11  0.22  0.17 ]\n",
      " [0.152 0.11  0.    0.152 0.144 0.168 0.144 0.13 ]\n",
      " [0.132 0.    0.136 0.132 0.123 0.146 0.123 0.207]]\n",
      "the -> NULL\n",
      "house -> etxea\n",
      "the -> urdina\n",
      "is -> da\n",
      "is -> nire\n",
      "my -> zuria\n",
      "was -> zen\n",
      "dog -> txakurra\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "def create_dataset(sents):\n",
    "    dict = {}\n",
    "    idict = {}\n",
    "    idSents = []\n",
    "    id = 1\n",
    "    for sent in sents:\n",
    "        idSent = []\n",
    "        for word in sent.split():\n",
    "            if word not in dict:\n",
    "                dict[word] = id\n",
    "                idict[id] = word\n",
    "                idSent.append(id)\n",
    "                id += 1                 \n",
    "            else:\n",
    "                idSent.append(dict[word])\n",
    "        idSents.append(idSent)\n",
    "    return dict, idict, idSents\n",
    "    \n",
    "srcSents = ['the house is blue','my house was white','my dog is white','the dog was blue']\n",
    "trgSents = ['etxea urdina da','nire etxea zuria zen','nire txakurra zuria da','txakurra urdina zen']\n",
    "\n",
    "srcDict, isrcDict, srcData = create_dataset(srcSents)\n",
    "trgDict, itrgDict, trgData = create_dataset(trgSents)\n",
    "\n",
    "print(f'srcData = {srcData}')\n",
    "print(f'trgData = {trgData}')\n",
    "\n",
    "# M1 dictionary initialise with uniform distro\n",
    "M1Dict = np.zeros((len(trgDict)+1,len(srcDict)),dtype=float)\n",
    "for trgWord in range(len(M1Dict)):\n",
    "    M1Dict[trgWord] = 1.0/len(srcDict)\n",
    "print(f'M1Dict = {M1Dict}')\n",
    "\n",
    "\n",
    "for iter in range(5):\n",
    "    newM1Dict = np.zeros((len(trgDict)+1,len(srcDict)),dtype=float)\n",
    "    for n in range(len(srcData)): \n",
    "        # E-step\n",
    "        a = np.zeros((len(srcData[n]), len(trgData[n])+1),dtype=float)\n",
    "        for j in range(len(srcData[n])):\n",
    "            # NULL word\n",
    "            a[j][0] = M1Dict[0][srcData[n][j]-1]\n",
    "            suma = a[j][0]\n",
    "            for i in range(len(trgData[n])):\n",
    "                a[j][i+1] = M1Dict[trgData[n][i]][srcData[n][j]-1]\n",
    "                suma += a[j][i+1]\n",
    "            a[j][0] /= suma\n",
    "            for i in range(len(trgData[n])):\n",
    "                a[j][i+1] /= suma\n",
    "        #print(f'a =\\n{a}')\n",
    "        # M-step\n",
    "        for j in range(len(srcData[n])):\n",
    "            newM1Dict[0][srcData[n][j]-1] += a[j][0]\n",
    "            for i in range(len(trgData[n])):\n",
    "                newM1Dict[trgData[n][i]][srcData[n][j]-1] += a[j][i]\n",
    "        #print(f'newM1Dict = {newM1Dict}')\n",
    "\n",
    "    # Normalise to obtain probabilities\n",
    "    for trgWord in range(len(M1Dict)):\n",
    "        suma = np.sum(newM1Dict[trgWord])\n",
    "        for srcWord in range(len(M1Dict[trgWord])):\n",
    "            newM1Dict[trgWord][srcWord] /= suma\n",
    "\n",
    "    # Update M1 dictionary\n",
    "    M1Dict = newM1Dict\n",
    "print(f'M1Dict = {M1Dict}')\n",
    "\n",
    "print(f'{isrcDict[np.argmax(M1Dict[0])+1]} -> NULL')\n",
    "for trgWord in range(1,len(M1Dict)):\n",
    "    print(f'{isrcDict[np.argmax(M1Dict[trgWord])+1]} -> {itrgDict[trgWord]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other word-based models\n",
    "\n",
    "<ul>\n",
    "<li>IBM research group proposed models 1 through 5: alignment distro (model 2), fertility (model 3)</li>\n",
    "<li>HMM alignment model</li>\n",
    "<li>Mixture models</li>\n",
    "<li>etc.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"https://aclanthology.org/J93-2003.pdf\" target=\"_blank\">P.F. Brown et al. The Mathematics of Statistical Machine Translation: Parameter Estimation, Computational Linguistics, 1993.</a></li>\n",
    "<li><a href=\"https://kevincrawfordknight.github.io/papers/wkbk-rw.pdf\" target=\"_blank\">K. Knight. A Statistical MT Tutorial Workbook, August 1999.</a></li>\n",
    "<li><a href=\"https://github.com/moses-smt/giza-pp\" target=\"_blank\">F. Och. GIZA++ toolkit and the mkcls tool.</a></li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
