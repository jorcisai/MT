{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from data\n",
    "\n",
    "<ul>\n",
    "<li><b>Monolingual data</b></li>\n",
    "    Ex.: Mary did not slap the green witch.\n",
    "<li><b>Multilingual data</b></li>\n",
    "    Ex.: Mary did not slap the green witch. Mary no dió una botefada a la bruja verde.\n",
    "<li><b>Parallel data</b></li>\n",
    "<ul>\n",
    "<li><b>Text-To-Text.</b></li>\n",
    "    Ex.: Mary did not slap the green witch. <b>||</b> Mary no dió una botefada a la bruja verde.\n",
    "<li><b>Speech-To-Text.</b> Automatic speech recognition or speech translation</li> \n",
    "<li><b>Text-To-Speech.</b> Speech synthesis</li>\n",
    "<li><b>Speech-To-Speech</b></li>\n",
    "</ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from parallel data: text-to-text\n",
    "\n",
    "Example of parallel text:\n",
    "<table>\n",
    "<tr><td>the house is blue</td><td>etxea urdina da</td></tr>\n",
    "<tr><td>my house was white</td><td>nire etxea zuria zen</td></tr>\n",
    "<tr><td>my dog is white</td><td>nire txakurra zuria da</td></tr>\n",
    "<tr><td>the dog was blue</td><td>txakurra urdina zen</td></tr>\n",
    "</table>\n",
    "\n",
    "Exercise: Can you identify which words are mutual translations? That is, define a bilingual dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "<table>\n",
    "<tr><td>my</td><td>nire</td></tr>\n",
    "<tr><td>house</td><td>etxea</td></tr>\n",
    "<tr><td>is</td><td>da</td></tr>\n",
    "<tr><td>blue</td><td>urdina</td></tr>\n",
    "<tr><td>dog</td><td>txakurra</td></tr>\n",
    "<tr><td>was</td><td>zen</td></tr>\n",
    "<tr><td>the</td><td>NULL</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>The concept of <b>alignment</b> between source and target words naturally arises.</li>\n",
    "<li>If alignments were available, it would be straightforward to derive a bilingual dictionary.</li>\n",
    "<li>Can we automatically learn word alignments from parallel text?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-based alignment models\n",
    "\n",
    "\n",
    "Let $x = x_1 \\cdots x_{|x|} = x_1^{|x|}$ and $y = y_1 \\cdots y_{|y|} = y_1^{|y|}$ be source and target sentences that are mutual translations. The variables $x_j$ and $y_i$ denote the $j$-th source word and the $i$-th target word, respectively. For the sake of clarity, let $J=|x|$ and $I=|y|$ be the number of source and target words, respectively.\n",
    "\n",
    "Let $a = a_1 \\cdots a_J$ be an alignment variable that assigns each target position to a source position. That is, $a_j \\in \\{1,\\cdots,I\\}$. For example, in the first sentence above, $a=(1, 2, 4, 3)$.\n",
    "\n",
    "More precisely, a ficticius target position $i=0$ (NULL word) is defined to account for those positions in the source sentence that are not aligned to any target position. Thus, $a_i \\in \\{0, 1,\\cdots,I\\}$. So, the last sentence would be $a=(0, 2, 4, 3)$.\n",
    "\n",
    "The alignment is considered a hidden variable, so that we sum over all its possible values:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(y \\mid x) &= P(y, I \\mid x)\\\\%\n",
    "            &= P(I \\mid x) \\, P(y \\mid I, x)\\\\\n",
    "            &= P(I \\mid x) \\sum_a P(y, a \\mid I, x)\\\\%\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(y, a \\mid I, x) &= \\prod_{i=1}^I P(y_i, a_i \\mid x, y_1^{i-1}, a_1^{i-1})\\\\%\n",
    "                  &= \\prod_{i=1}^I P(y_i \\mid x, y_1^{i-1}, a_1^{i}) \\, P(a_i \\mid x, y_1^{i-1}, a_1^{i-1})%\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "### Model 1\n",
    "\n",
    "Assumptions and model parameters:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(y_i \\mid x, y_1^{i-1}, a_1^{i})   &:= p(y_i \\mid x_{a_i})\\\\ \n",
    "P(a_i \\mid x, y_1^{i-1}, a_1^{i-1}) &:= \\frac{1}{J+1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Model 1 is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(y \\mid x) &\\sim \\sum_a \\prod_{i=1}^I \\frac{1}{J+1} \\, p(y_i \\mid x_{a_i})\\\\%\n",
    "            &=       \\prod_{i=1}^I \\sum_{a_i=0}^J \\frac{1}{J+1} \\, p(y_i \\mid x_{a_i})\\\\%\n",
    "            &= \\frac{1}{(J+1)^I} \\, \\prod_{i=1}^I \\sum_{a_i=0}^J p(y_i \\mid x_{a_i})\\\\%\n",
    "            &= \\frac{1}{(J+1)^I} \\, \\prod_{i=1}^I \\sum_{j=0}^J p(y_i \\mid x_j)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Parameter optimization of log-likelihood by EM algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{E step}: a_{nij} &= \\frac{p(y_{ni} \\mid x_{nj})}{\\sum_{j'} p(y_{ni} \\mid x_{nj'})}\\\\%\n",
    "\\text{M step}: p(u \\mid v) &\\sim  \\sum_n \\sum_{i:y_{ni}=u} \\sum_{j:x_{nj}=v} a_{nij}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(  was | x ) = 0.11 (None) 0.02 (nire) 0.02 (etxea) 0.02 (zuria) 0.83 (zen) 0.03 (txakurra) 0.01 (urdina) \n",
      "p(  the | x ) = 0.21 (None) 0.05 (etxea) 0.48 (urdina) 0.05 (da) 0.05 (txakurra) 0.05 (zen) \n",
      "p( blue | x ) = 0.21 (None) 0.05 (etxea) 0.48 (urdina) 0.05 (da) 0.05 (txakurra) 0.05 (zen) \n",
      "p(house | x ) = 0.11 (None) 0.83 (etxea) 0.01 (urdina) 0.03 (da) 0.02 (nire) 0.02 (zuria) 0.02 (zen) \n",
      "p(  dog | x ) = 0.11 (None) 0.02 (nire) 0.83 (txakurra) 0.02 (zuria) 0.02 (da) 0.01 (urdina) 0.03 (zen) \n",
      "p(white | x ) = 0.06 (None) 0.46 (nire) 0.01 (etxea) 0.46 (zuria) 0.01 (zen) 0.01 (txakurra) 0.01 (da) \n",
      "p(   is | x ) = 0.11 (None) 0.03 (etxea) 0.01 (urdina) 0.83 (da) 0.02 (nire) 0.02 (txakurra) 0.02 (zuria) \n",
      "p(   my | x ) = 0.06 (None) 0.46 (nire) 0.01 (etxea) 0.46 (zuria) 0.01 (zen) 0.01 (txakurra) 0.01 (da) \n",
      "['etxea', 'urdina', 'da'] > ['the', 'house', 'is', 'blue']: 0-1 1-0 2-2 3-1\n",
      "['nire', 'etxea', 'zuria', 'zen'] > ['my', 'house', 'was', 'white']: 0-2 1-1 2-3 3-2\n",
      "['nire', 'txakurra', 'zuria', 'da'] > ['my', 'dog', 'is', 'white']: 0-2 1-1 2-3 3-2\n",
      "['txakurra', 'urdina', 'zen'] > ['the', 'dog', 'was', 'blue']: 0-1 1-0 2-2 3-1\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate import AlignedSent, IBMModel1\n",
    "\n",
    "euText = ['etxea urdina da','nire etxea zuria zen','nire txakurra zuria da','txakurra urdina zen']\n",
    "enText = ['the house is blue', 'my house was white','my dog is white', 'the dog was blue']\n",
    "\n",
    "# Source language is Euskera and target language is English\n",
    "corpus = []\n",
    "for enSent, euSent in zip(enText,euText):\n",
    "    corpus.append(AlignedSent(enSent.split(),euSent.split()))\n",
    "\n",
    "# Training M1 model for 5 iterations \n",
    "# p(trg_word | src_word): m1.translation_table[trgWord][srcWord]\n",
    "m1 = IBMModel1(corpus, 5)\n",
    "\n",
    "for trgWord in m1.translation_table:\n",
    "    print(f'p({trgWord:>5} | x ) = ',end=\"\")\n",
    "    for srcWord in m1.translation_table[trgWord]:\n",
    "        print(f'{m1.translation_table[trgWord][srcWord]:.2f} ({srcWord}) ',end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "# Computing best alignment\n",
    "m1.align_all(corpus)\n",
    "for sentencePair in corpus: print(f'{sentencePair.mots} > {sentencePair.words}: {sentencePair.alignment}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "\n",
    "Assumptions and model parameters:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(y_i \\mid x, y_1^{i-1}, a_1^{i})   &:= p(y_i \\mid x_{a_i})\\\\ \n",
    "P(a_i \\mid x, y_1^{i-1}, a_1^{i-1}) &:= p(a_i \\mid i, J, I)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Model 2 is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(y \\mid x) &\\sim \\prod_{i=1}^I \\sum_{a_i=0}^J p(a_i \\mid i, J, I) \\, p(y_i \\mid x_{a_i})\\\\%\n",
    "            &=    \\prod_{i=1}^I \\sum_{j=0}^J   p(j \\mid i, J, I) \\, p(y_i \\mid x_j)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Parameter optimization of log-likelihood by EM algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{E step}: a_{nij} &= \\frac{p(j \\mid i, J, I) \\, p(y_{ni} \\mid x_{nj})}{\\sum_{j'} p(j' \\mid i, J, I) \\, p(y_{ni} \\mid x_{nj'})}\\\\%\n",
    "\\text{M step}: p(j \\mid i, J, I) & \\sim  \\sum_{n:x_n=J \\wedge y_n=I} a_{nij} \\\\%\n",
    "               p(u \\mid v) &\\sim  \\sum_n \\sum_{i:y_{ni}=u} \\sum_{j:x_{nj}=v} a_{nij}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(j | i = 1, J = 4, I = 4) = 0.00 (j = 0) 0.50 (j = 1) 0.00 (j = 2) 0.50 (j = 3) 0.00 (j = 4) \n",
      "p(j | i = 2, J = 4, I = 4) = 0.00 (j = 0) 0.00 (j = 1) 1.00 (j = 2) 0.00 (j = 3) 0.00 (j = 4) \n",
      "p(j | i = 3, J = 4, I = 4) = 0.00 (j = 0) 0.00 (j = 1) 0.00 (j = 2) 0.00 (j = 3) 1.00 (j = 4) \n",
      "p(j | i = 4, J = 4, I = 4) = 0.00 (j = 0) 0.50 (j = 1) 0.00 (j = 2) 0.50 (j = 3) 0.00 (j = 4) \n"
     ]
    }
   ],
   "source": [
    "from nltk.translate import AlignedSent, IBMModel2\n",
    "\n",
    "# Training M1 model for 10 iterations followed by M2 model for 5 iterations\n",
    "# p(j | i, J, I): m1.alignment_table[j][i][J][I]\n",
    "m2 = IBMModel2(corpus, 5)\n",
    "\n",
    "J=4; I=4\n",
    "for i in range(1,I+1):\n",
    "    print(f'p(j | i = {i}, J = {J}, I = {I}) = ',end=\"\")\n",
    "    for j in range(J+1):\n",
    "        print(f'{m2.alignment_table[j][i][4][4]:.2f} (j = {j}) ',end=\"\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other word-based models\n",
    "\n",
    "<ul>\n",
    "<li>In addition to models 1 and 2, the IBM research group proposed models 3 through 5</li>\n",
    "<li>HMM alignment model</li>\n",
    "<li>Mixture models</li>\n",
    "<li>etc.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"https://aclanthology.org/J93-2003.pdf\" target=\"_blank\">P.F. Brown et al. The Mathematics of Statistical Machine Translation: Parameter Estimation, Computational Linguistics, 1993.</a></li>\n",
    "<li><a href=\"https://kevincrawfordknight.github.io/papers/wkbk-rw.pdf\" target=\"_blank\">K. Knight. A Statistical MT Tutorial Workbook, August 1999.</a></li>\n",
    "<li><a href=\"https://github.com/moses-smt/giza-pp\" target=\"_blank\">F. Och. GIZA++ toolkit and the mkcls tool.</a></li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
