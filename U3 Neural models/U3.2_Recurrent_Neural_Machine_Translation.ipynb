{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Machine Translation\n",
    "\n",
    "From LM to MT: from monolingual to cross-lingual word prediction\n",
    "\n",
    "Early proposals with syncronized RNN [1]\n",
    "\n",
    "Non-syncronized RNN: Encoder-Decoder architecture [2]\n",
    "\n",
    "<img src=\"Sutskever2014_EncoderDecoder.svg\" width=\"1000\"/>\n",
    "\n",
    "A source LM (encoder) whose hidden state is transfered to a target LM (decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding (Beam Search)\n",
    "\n",
    "For a source sentence $x_1^J$ search for a target sentence\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y}_1^{\\hat{I }} &= \\operatorname*{argmax}_{y_1^I} P(y_1^I\\mid x_1^J)\\\\\n",
    "                     &= \\operatorname*{argmax}_{y_1^I} \\prod_{i=1}^I p(y_i\\mid y_1^{i-1},x_1^J)\\\\\n",
    "                     &= \\operatorname*{argmax}_{y_1^I} \\prod_{i=1}^I p(y_i\\mid y_1^{i-1},u(x_1^J))\n",
    "\\end{align}\n",
    "$$\n",
    "where $u(x_1^J)$ is a hidden-state representation of $x_1^J$.\n",
    "\n",
    "Search for the most likely translation using a simple left-to-right beam search decoder\n",
    "\n",
    "At each timestep each partial hypothesis (prefix translations) is extended with every word in the vocabulary (in practice, the k most probable tokens).\n",
    "\n",
    "Resulting in at most $k \\times k$ new prefixes of size increased by 1.\n",
    "\n",
    "Then among these, the $k$ prefixes with the highest likelihood are maintaned\n",
    "\n",
    "When the symbol &lt;eos&gt; appears in a hypothesis, it is considered to be complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on WMT14 English-to-French [2]:\n",
    "\n",
    "|Systems|BLEU|\n",
    "|:------|---:|\n",
    "|Baseline (PB+Neural LM) | 33.3|\n",
    "|Ensemble of 5 LSTMs | 34.8|\n",
    "|Reescoring 1000-best with ensemble of 5 LSTMs | 36.5|\n",
    "|Moses for WMT14 | 37.0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with attention (Bahdanau et al.) [3]\n",
    "\n",
    "Conventional RNN *squash* all the source sentence information into a fixed-length vector\n",
    "\n",
    "RNN tend to to better represent recent inputs\n",
    "\n",
    "RNN with attention encode the input sentence into a sequence of vectors $h_1^J$ \n",
    "\n",
    "They select a subset of these vectors $h_1^J$ adaptively (attention mechanism) while decoding\n",
    "$$\n",
    "\\hat{y}_1^{\\hat{I }} = \\operatorname*{argmax}_{y_1^I} \\prod_{i=1}^I p(y_i\\mid y_1^{i-1},u_i)\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\begin{align}\n",
    "\\notag u_i &= a(h^e_1, \\cdots, h^e_J, h^d_{i-1})\\\\ \n",
    "\\notag     &= \\sum_{j=1}^{J} \\alpha(h^e_1, \\cdots, h^e_J, h^d_{i-1})\\,h^e_j\n",
    "\\end{align}    \n",
    "$$\n",
    "being $\\alpha(h^e_j, h^d_{i-1})$ a similarity measure (alignment) between the encoder state at position $j$ and \n",
    "the decoder state at position $i-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bahdanau et al. define this similarity measure as a probability distribution over source positions\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha(h^e_1, \\cdots, h^e_J, h^d_{i-1}) &= \\alpha_j(h^e_1, \\cdots, h^e_J, h^d_{i-1})\\\\\n",
    "                         &= \\alpha_{ij}\\\\\n",
    "                         &= \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{J} \\exp(e_{ik})}\\\\\n",
    "                         &= \\mathcal{S}(e_{ij})\n",
    "\\end{align}\n",
    "$$\n",
    "being\n",
    "$$\n",
    "e_{ij} = v_a^t \\tanh \\left( W_a h^d_{i-1} + U_a h^e_j \\right)\n",
    "$$\n",
    "where $v_a \\in \\mathbb{R}$, $W_a \\in \\mathbb{R}^{n \\times n}$ and $U_a \\in \\mathbb{R}^{n \\times 2n}$\n",
    "\n",
    "Consequently, the hidden state of the RNN decoder $h^d_i$ is enriched with $u_i$ as\n",
    "$$\n",
    "h^d_i = F(h^d_{i-1}, y_{i−1}, u_i)\n",
    "$$\n",
    "and the output layer computing the probability over the vocabulary also incorporates $u_i$ and the residual connection $y_{i-1}$ \n",
    "$$\n",
    "p(y_i\\mid y_1^{i-1},u_i) = g(h^d_i, y_{i−1}, u_i)\n",
    "$$\n",
    "Indeed, the RNN hidden state at the encoder $h_j$ is the concatenation of the hidden state of a forward and a backward RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bahdanau2015_EncoderDecoder.svg\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results PBMT vs NMT (2017)\n",
    "<center>\n",
    "<img src=\"PBMT-NMT2017.png\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with attention (Luong et al.) [4]\n",
    "\n",
    "Simplification and generalisation over [3]\n",
    "\n",
    "  * Unidirectional encoder\n",
    "  * Alternative attention mechanism: $ e_{ij} = \\left\\{ \\begin{matrix} \\left(h_i^d\\right)^t \\, h^e_j\\\\\\left(h_i^d\\right)^t \\, W_a \\, h^e_j\\\\ W_a \\, [h^d_i;h^e_j]  \\end{matrix} \\right.$\n",
    "\n",
    "All in all, in 2014, NMT systems still behind SOTA phrase-based systems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Luong2015_EncoderDecoder.svg\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ol>\n",
    "<li><a href=\"https://www.isca-archive.org/eurospeech_1997/castano97_eurospeech.pdf\" target=\"_blank\">M.A. Castaño and F. Casacuberta. A Connectionist Approach to Machine Translation, EuroSpeech 1997.</a></li>\n",
    "<li><a href=\"https://papers.nips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf\" target=\"_blank\">I. Sutskever et al. Sequence to Sequence Learning with Neural Networks, NIPS 2014.</a></li>\n",
    "<li><a href=\"https://arxiv.org/pdf/1409.0473\" target=\"_blank\">D. Bahdanau et al. Neural Machine Translation by Jointly Learning to Align and Translate, ICLR 2015.</a></li>\n",
    "<li><a href=\"https://aclanthology.org/D15-1166.pdf\" target=\"_blank\">M. Luong et al. Effective Approaches to Attention-based Neural Machine Translation, EMNLP 2015.</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tfcuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
