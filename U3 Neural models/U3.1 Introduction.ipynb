{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework\n",
    "\n",
    "Neural MT is based on the modelisation of  $P(y \\mid x)$ with a neural network (NN) to perform\n",
    "\n",
    "$$\\hat{y} = \\argmax_{y} P(y \\mid x)$$\n",
    "\n",
    "where $x$ is a source sentence and $y$ is a target sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminars\n",
    "\n",
    "Processing a sequence of words by a NN is a challenging task because of its discrete nature \n",
    "\n",
    "Previous works had to work with very limited vocabulary (a few tens of words):\n",
    "  * Language modelling with Elman (recurrent) NN [1]\n",
    "  * Machine translation with RNN [2]\n",
    "\n",
    "Need of reducing the vocabulary size to reduce computational requirements\n",
    "  * Tokeniser: language dependent and not reversibly convertible\n",
    "  * Unsupervised reversible tokenisers: Byte-Pair Encoding [7], SentencePiece [8], etc.\n",
    "\n",
    "Need of mapping word representation to a continuous space to be processed by a NN\n",
    "  * Unsuccessful attempts by using bag-of-words, latent semantic indexing, word classes, clustering, etc.\n",
    "  * Learning a feature vector for each word together with the task \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised reversible tokenisers\n",
    "\n",
    "### Byte-pair Encoding (BPE)\n",
    "  * It iteratively merges the most frequent pair of symbols up to a maximum number of merge operations\n",
    "  * It starts with a symbol vocabulary that is the character vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n",
      "lowest\n",
      "lower\n",
      "new\n",
      "newest\n",
      "newer\n",
      "wider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|##########################################| 3/3 [00:00<00:00, 12180.94it/s]\n",
      "#version: 0.2\n",
      "w e\n",
      "n e\n",
      "l o\n",
      "-----------\n",
      "lo@@ w\n",
      "lo@@ we@@ s@@ t\n",
      "lo@@ we@@ r\n",
      "ne@@ w\n",
      "ne@@ we@@ s@@ t\n",
      "ne@@ we@@ r\n",
      "w@@ i@@ d@@ e@@ r\n"
     ]
    }
   ],
   "source": [
    "!echo -e  \"low\\nlowest\\nlower\\nnew\\nnewest\\nnewer\\nwider\" | tee train.txt\n",
    "!subword-nmt learn-bpe --min-frequency 1 -s 3 < train.txt > codes.txt\n",
    "!cat codes.txt; echo \"-----------\"\n",
    "!subword-nmt apply-bpe -c codes.txt < train.txt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised reversible tokenisers\n",
    "\n",
    "### SentencePiece\n",
    "  * Integrates the implementation of the unigram (*uni-subword*) language model [9] \n",
    "  * Capable of outputing multiple subword segmentations with probabilities\n",
    "  * The most probable subword segmentations can be considered for training and decoding\n",
    "  * No need of previous tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\t0\n",
      "<s>\t0\n",
      "</s>\t0\n",
      "r\t-1.94426\n",
      "s\t-2.44426\n",
      "t\t-2.44426\n",
      "▁lowe\t-2.47125\n",
      "▁newe\t-2.47126\n",
      "e\t-3.31599\n",
      "▁new\t-3.37828\n",
      "▁low\t-3.37829\n",
      "o\t-3.44406\n",
      "n\t-3.44416\n",
      "d\t-3.44426\n",
      "i\t-3.44426\n",
      "l\t-3.44426\n",
      "w\t-3.44426\n",
      "▁\t-3.44426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=train.txt --model_prefix=train --vocab_size=18\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: train.txt\n",
      "  input_format: \n",
      "  model_prefix: train\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 18\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: train.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 7 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=40\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=11\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 7 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=32\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 24 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 7\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 7\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 7 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=12 obj=9.0722 num_tokens=18 num_tokens/piece=1.5\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=12 obj=7.02774 num_tokens=18 num_tokens/piece=1.5\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: train.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: train.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['▁lowe', 's', 't'],\n",
       " ['▁low', 'e', 's', 't'],\n",
       " ['▁', 'l', 'o', 'w', 'e', 's', 't']]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "params = ('--input=train.txt ''--model_prefix=train ''--vocab_size=18')\n",
    "spm.SentencePieceTrainer.Train(params)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "!cat train.vocab\n",
    "sp.Load('train.model')\n",
    "sp.nbest_encode('lowest', nbest_size=3, out_type=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning a word feature vector, a.k.a. word embedding, jointly with the language modelling task [3]\n",
    "  * Feedforward NN achieved 20% perplexity relative reduction w.r.t. n-grams\n",
    "  * Maximise log-likelihood = minimise cross-entropy = minimise perplexity\n",
    "  * Still limited vocabulary size (tens of thousands) and running words (a few millions)\n",
    "  * A few tens of hidden units -> training time: one week per epoch in 40 CPUs \n",
    "\n",
    "<img src=\"MLPLM.svg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz; graphviz.Source('''\n",
    "digraph { \n",
    "    concentrate=True;\n",
    "    rankdir=BT;\n",
    "    node [shape=record];\n",
    "    WE [label=\"Word embedding\\n|{output:|input:}|{{m}|{V}}\"];\n",
    "    HL [label=\"MLP (Hidden layer weights)\\n|{output:|input:}|{{h}|{m · (n-1)}}\"];\n",
    "    OL [label=\"Softmax (Output weights)\\n|{output:|input:}|{{V}|{h + m · (n-1)}}\"];\n",
    "    WE -> HL\n",
    "    HL -> OL\n",
    "    WE -> OL\n",
    "    node [shape=circle];\n",
    "    wb [label=<W<sub>i-n-1</sub>>,fixedsize=true,width=0.7];\n",
    "    wm [label=\"...\",fixedsize=true,width=0.7];\n",
    "    we [label=<W<sub>i-1</sub>>,fixedsize=true,width=0.7];\n",
    "    wo  [label=<W<sub>i</sub>>,fixedsize=true,width=0.7];\n",
    "    wb -> WE\n",
    "    wm -> WE\n",
    "    we -> WE\n",
    "    OL  -> wo\n",
    "}''').render(filename='MLPLM', format='svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modelling on real-tasks with RNN [4]\n",
    "  * From backpropagation to backpropagation through time\n",
    "  * Various optimisations to scale up running words in training (hundreds of millions)\n",
    "  * Numerical stability issues: double precision and gradient explosion (truncation)\n",
    "  * Relative reduction of 20% in state-of-the-art ASR tasks\n",
    "  \n",
    "  <img src=\"RNNLM.svg\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz as G\n",
    "\n",
    "# boolean variables to denote dense or sparse connections between layers\n",
    "DENSE = True\n",
    "SPARSE = False\n",
    "\n",
    "\n",
    "TIMESTEPS = 5\n",
    "TIME_OFFSET = 3\n",
    "\n",
    "unrolled = G.Digraph(node_attr={'shape':'circle', 'fixedsize':'true'}, graph_attr={'style':'invis', 'rankdir':'BT', 'color':'transparent'})\n",
    "\n",
    "i=0\n",
    "for step in range(TIMESTEPS+2):\n",
    "    if step == 0 or step == TIMESTEPS+1:\n",
    "        with unrolled.subgraph(name='cluster_'+str(i)) as c:\n",
    "            c.node('a'+str(step), '', color='transparent')\n",
    "            c.node('b'+str(step), '...', color='transparent')\n",
    "            c.node('c'+str(step), '...', color='transparent') \n",
    "            c.node('d'+str(step), '...', color='transparent')\n",
    "            c.edge('a'+str(step), 'b'+str(step), style='invis') \n",
    "            c.edge('b'+str(step), 'c'+str(step), style='invis')\n",
    "            c.edge('c'+str(step), 'd'+str(step), style='invis')\n",
    "    else:\n",
    "        with unrolled.subgraph(name='cluster_'+str(i)) as c:\n",
    "            c.node('a'+str(step), '', color='transparent');\n",
    "            c.node('b'+str(step), 'WE')\n",
    "            #c.node('c'+str(step), 't'+'{:=+d}'.format(TIME_OFFSET-step) if TIME_OFFSET-step else 't')\n",
    "            c.node('c'+str(step), '')\n",
    "            c.node('d'+str(step), 'SM');\n",
    "            c.node('e'+str(step), '<w<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step+1)+'</sub>>' if TIME_OFFSET-step+1 else '<w<sub>'+'t'+'</sub>>', color='transparent');\n",
    "            c.edge('a'+str(step), 'b'+str(step), label='<w<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step)+'</sub>>' if TIME_OFFSET-step else '<w<sub>'+'t'+'</sub>>'); \n",
    "            c.edge('b'+str(step), 'c'+str(step), label='<w<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step)+'</sub>>' if TIME_OFFSET-step else '<w<sub>'+'t'+'</sub>>'); \n",
    "            c.edge('c'+str(step), 'd'+str(step), label='<y<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step)+'</sub>>' if TIME_OFFSET-step else '<y<sub>'+'t'+'</sub>>');\n",
    "            c.edge('d'+str(step), 'e'+str(step), label='');\n",
    "            \n",
    "for step in range(1, TIMESTEPS+2):\n",
    "    unrolled.edge('c'+str(step-1), 'c'+str(step), label='<h<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step)+'</sub>>' if TIME_OFFSET-step else '<h<sub>'+'t'+'</sub>>', constraint='false', dir='back', color='black')\n",
    "\n",
    "unrolled.render(filename='RNNLM', format='svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of RNN\n",
    "\n",
    "Output depens not only on near but also far context in a sentence\n",
    "\n",
    "Numerical instability issues with gradients (propagated error) vanishing and exploiding\n",
    "\n",
    "Solution: Long Short-Term Memory (LSTM) [5] or Gated Recurrent Units (GRU) [6] cells\n",
    "\n",
    "## LSTM cell\n",
    "\n",
    "It replaces the nodes in the hidden layer\n",
    "\n",
    "It explicitly models a memory state to retain near/far context\n",
    "\n",
    "Output and memory state change depends on parametrised *gates*:\n",
    "  * input gate: how much new input changes memory state\n",
    "  * forget gate: how much of prior memory state is retained\n",
    "  * output gate: how strongly memory state is passed on to next layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ol>\n",
    "<li><a href=\"https://onlinelibrary.wiley.com/doi/epdf/10.1207/s15516709cog1402_1\" target=\"_blank\">J. Elman. Finding Structure in Time, Cognitive Science 1990.</a></li>\n",
    "<li><a href=\"https://www.isca-archive.org/eurospeech_1997/castano97_eurospeech.pdf\" target=\"_blank\">M.A. Castaño and F. Casacuberta. A Connectionist Approach to Machine Translation, EuroSpeech 1997.</a></li>\n",
    "<li><a href=\"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\" target=\"_blank\">Y. Bengio et al. A Neural Probabilistic Language Model, Journal of Machine Learning Research 2003.</a></li>\n",
    "<li><a href=\"https://www.fit.vut.cz/study/phd-thesis-file/283/283.pdf\" target=\"_blank\">T. Mikolov. Statistical Language Models based on Neural Networks, Ph.D. Thesis 2012.</a></li>\n",
    "<li><a href=\"https://www.bioinf.jku.at/publications/older/2604.pdf\" target=\"_blank\">S. Hochreiter and J. Schmidhuber. Long short-term memory, Neural Computation 1997.</a></li>\n",
    "<li><a href=\"https://arxiv.org/pdf/1406.1078\" target=\"_blank\">K. Cho et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, ACL 2014.</a></li>\n",
    "<li><a href=\"https://aclanthology.org/P16-1162.pdf\" target=\"_blank\">R. Sennrich et al. Neural Machine Translation of Rare Words with Subword Units, ACL 2016.</a></li>\n",
    "<li><a href=\"https://aclanthology.org/D18-2012.pdf\" target=\"_blank\">T. Kudo and J. Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing, EMNLP 2018.</a></li>\n",
    "<li><a href=\"https://aclanthology.org/P18-1007.pdf\" target=\"_blank\">T. Kudo. Subword Regularization: Improving Neural Network Translation Models\n",
    "with Multiple Subword Candidates, ACL 2018.</a></li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tfcuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
