{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework\n",
    "\n",
    "Neural MT is based on the modelisation of  $P(y \\mid x)$ with a neural network (NN) to perform\n",
    "\n",
    "$$\\hat{y} = \\argmax_{y} P(y \\mid x)$$\n",
    "\n",
    "where $x$ is a source sentence and $y$ is a target sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminars\n",
    "\n",
    "Processing a sequence of words by a NN is a challenging task because of its discrete nature \n",
    "\n",
    "Previous works had to work with very limited vocabulary (a few tens of words):\n",
    "  * Language modelling with Elman (recurrent) NN [1]\n",
    "  * Machine translation with RNN [2]\n",
    "\n",
    "Need of mapping word representation to a continuous space to be processed by a NN\n",
    "  * Unsuccessful attempts by using bag-of-words, latent semantic indexing, word classes, clustering, etc.\n",
    "  * Learning a feature vector for each word together with the task \n",
    "\n",
    "Learning a word feature vector, a.k.a. word embedding, jointly with the language modelling task [3]\n",
    "  * Feedforward NN achieved 20% perplexity relative reduction w.r.t. n-grams\n",
    "  * Maximise log-likelihood = minimise cross-entropy = minimise perplexity\n",
    "  * Still limited vocabulary size (tens of thousands) and running words (a few millions)\n",
    "  * A few tens of hidden units -> training time: one week per epoch in 40 CPUs \n",
    "\n",
    "<img src=\"MLPLM.svg\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz; graphviz.Source('''\n",
    "digraph { \n",
    "    concentrate=True;\n",
    "    rankdir=BT;\n",
    "    node [shape=record];\n",
    "    WE [label=\"Word embedding\\n|{output:|input:}|{{m}|{V}}\"];\n",
    "    HL [label=\"MLP (Hidden layer weights)\\n|{output:|input:}|{{h}|{m · (n-1)}}\"];\n",
    "    OL [label=\"Softmax (Output weights)\\n|{output:|input:}|{{V}|{h + m · (n-1)}}\"];\n",
    "    WE -> HL\n",
    "    HL -> OL\n",
    "    WE -> OL\n",
    "    node [shape=circle];\n",
    "    wb [label=<W<sub>i-n-1</sub>>,fixedsize=true,width=0.7];\n",
    "    wm [label=\"...\",fixedsize=true,width=0.7];\n",
    "    we [label=<W<sub>i-1</sub>>,fixedsize=true,width=0.7];\n",
    "    wo  [label=<W<sub>i</sub>>,fixedsize=true,width=0.7];\n",
    "    wb -> WE\n",
    "    wm -> WE\n",
    "    we -> WE\n",
    "    OL  -> wo\n",
    "}''').render(filename='MLPLM', format='svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modelling on real-tasks with RNN [4]\n",
    "  * From backpropagation to backpropagation through time\n",
    "  * Various optimisations to scale up running words in training (hundreds of millions)\n",
    "  * Relative reduction of 20% in state-of-the-art ASR tasks\n",
    "  \n",
    "  <img src=\"RNNLM.svg\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz as G\n",
    "\n",
    "# boolean variables to denote dense or sparse connections between layers\n",
    "DENSE = True\n",
    "SPARSE = False\n",
    "\n",
    "\n",
    "TIMESTEPS = 5\n",
    "TIME_OFFSET = 3\n",
    "\n",
    "unrolled = G.Digraph(node_attr={'shape':'circle', 'fixedsize':'true'}, graph_attr={'style':'invis', 'rankdir':'BT', 'color':'transparent'})\n",
    "\n",
    "i=0\n",
    "for step in range(TIMESTEPS+2):\n",
    "    if step == 0 or step == TIMESTEPS+1:\n",
    "        with unrolled.subgraph(name='cluster_'+str(i)) as c:\n",
    "            c.node('a'+str(step), '', color='transparent')\n",
    "            c.node('b'+str(step), '...', color='transparent')\n",
    "            c.node('c'+str(step), '...', color='transparent') \n",
    "            c.node('d'+str(step), '...', color='transparent')\n",
    "            c.edge('a'+str(step), 'b'+str(step), style='invis') \n",
    "            c.edge('b'+str(step), 'c'+str(step), style='invis')\n",
    "            c.edge('c'+str(step), 'd'+str(step), style='invis')\n",
    "    else:\n",
    "        with unrolled.subgraph(name='cluster_'+str(i)) as c:\n",
    "            c.node('a'+str(step), '', color='transparent');\n",
    "            c.node('b'+str(step), 'WE')\n",
    "            #c.node('c'+str(step), 't'+'{:=+d}'.format(TIME_OFFSET-step) if TIME_OFFSET-step else 't')\n",
    "            c.node('c'+str(step), '')\n",
    "            c.node('d'+str(step), 'SM');\n",
    "            c.node('e'+str(step), '', color='transparent');\n",
    "            c.edge('a'+str(step), 'b'+str(step), label='<w<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step)+'</sub>>' if TIME_OFFSET-step else '<w<sub>'+'t'+'</sub>>'); \n",
    "            c.edge('b'+str(step), 'c'+str(step), label='<w<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step)+'</sub>>' if TIME_OFFSET-step else '<w<sub>'+'t'+'</sub>>'); \n",
    "            c.edge('c'+str(step), 'd'+str(step), label='<y<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step)+'</sub>>' if TIME_OFFSET-step else '<y<sub>'+'t'+'</sub>>');\n",
    "            c.edge('d'+str(step), 'e'+str(step), label='');\n",
    "\n",
    "for step in range(1, TIMESTEPS+2):\n",
    "    unrolled.edge('c'+str(step-1), 'c'+str(step), label='<h<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step)+'</sub>>' if TIME_OFFSET-step else '<h<sub>'+'t'+'</sub>>', constraint='false', dir='back', color='black')\n",
    "\n",
    "unrolled.render(filename='RNNLM', format='svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ol>\n",
    "<li><a href=\"https://onlinelibrary.wiley.com/doi/epdf/10.1207/s15516709cog1402_1\" target=\"_blank\">J. Elman. Finding Structure in Time, Cognitive Science 1990.</a></li>\n",
    "<li><a href=\"https://www.isca-archive.org/eurospeech_1997/castano97_eurospeech.pdf\" target=\"_blank\">M.A. Castaño and F. Casacuberta. A Connectionist Approach to Machine Translation, EuroSpeech 1997.</a></li>\n",
    "<li><a href=\"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\" target=\"_blank\">Y. Bengio et al. A Neural Probabilistic Language Model, Journal of Machine Learning Research 2003.</a></li>\n",
    "<li><a href=\"https://www.fit.vut.cz/study/phd-thesis-file/283/283.pdf\" target=\"_blank\">T. Mikolov. Statistical Language Models based on Neural Networks, Ph.D. Thesis 2012.</a></li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tfcuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
