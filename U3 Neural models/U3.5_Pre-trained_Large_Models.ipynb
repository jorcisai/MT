{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Large Models (PLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Training neural machine translation models from scratch is expensive\n",
    "\n",
    "Large bilingual corpora are necessary? \n",
    "\n",
    "For many task-specific translation models there are low resources\n",
    "\n",
    "Pre-trained large models have proved to be useful in many scenarios, but in MT?\n",
    "\n",
    "  * Combination of pre-trained encoders and pre-trained decoders?\n",
    "  * Multilingual pretrained language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "PLMs usually refer to Transformer-based architectural models with billions of parameters and trained on massive data\n",
    "\n",
    "PLMs exhibit strong capacities to understand natural language and solve complex tasks such as machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of pre-trained large models\n",
    "\n",
    "<center>\n",
    "<img src=\"EvolutionaryTreeLLMs2024.jpg\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling laws\n",
    "\n",
    "Model performance/capacity as a function of model and data size, and computational budget\n",
    "\n",
    "Given a limited compute budget, KM scaling laws favors a larger budget allocation in model size than the data\n",
    "size, while the Chinchilla scaling law argues that the two sizes should be increased in equal scales\n",
    "\n",
    "* Predictable scaling: loss decrease on smaller models convey to larger models\n",
    "* Task-level scaling: loss decrease does not always implies better performance in downstream tasks\n",
    "\n",
    "### Emergent abilities\n",
    "\n",
    "Abilities that are not present in small models but arise in large models\n",
    "\n",
    "* In-context learning. The LM provided with a natural language instruction and/or several task demonstrations, it can generate the expected output for the test instances by completing the word sequence of input text, without requiring additional training or gradient update\n",
    "\n",
    "* Instruction following. By fine-tuning with a mixture of multi-task datasets formatted via natural language descrip-\n",
    "tions without using explicit examples, LLMs are shown to perform well on unseen tasks that are also described in the form of instructions. \n",
    "<!--  According to the experiments in [67], instruction-tuned LaMDA-PT [68] started to significantly\n",
    "outperform the untuned one on unseen tasks when the model size reached 68B, but not for 8B or smaller model sizes. A recent study [69] found that a model size of 62B is at least required for PaLM to perform well on various tasks in four evaluation benchmarks (i.e., MMLU, BBH, TyDiQA and MGSM), though a much smaller size might suffice for some specific tasks (e.g., MMLU). >\n",
    "\n",
    "* Chain-of-thought (CoT): Step-by-step reasoning to solve complex tasks such mathematical word problem.\n",
    "<!-- CoT prompting can bring performance gains (on arithmetic reasoning benchmarks) when applied to PaLM and LaMDA variants with a model size larger than 60B, while its advantage over the standard prompting becomes more evident when the model size exceeds 100B -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources of PLMs\n",
    "\n",
    "### Publicly Available Model Checkpoints or APIs\n",
    "\n",
    "LlaMa family (Meta): Latest version is LlaMa 3.1\n",
    "\n",
    "* Largest version has 405B parameters, 15T training tokens, and an extended context\n",
    "window of 128K tokens \n",
    "* It achieves competitive performance against prominent closed-source LLMs, such as GPT-4, in various benchmark\n",
    "* It has greatly advanced the research progress of LLMs\n",
    "\n",
    "Mistral family: Mistral Large 2 (123B parameters)\n",
    "\n",
    "* Strong performance on various mainstream benchmarks (e.g., MMLU and GSM8k)\n",
    "\n",
    "Gemma family (Google): Gemma\n",
    "\n",
    "* Achieved excellent performance in multiple benchmarks (e.g., ARC-c, MMLU, and GSM8k)\n",
    "\n",
    "Other families: Qwen, GLM, Baichuan. etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commonly Used Corpora for Pre-training\n",
    "\n",
    "* Web pages: CommonCrawl, C4, RedPajama-Data, RefinedWeb, WebText\n",
    "\n",
    "* Books & Academic Data: Book Data, Academic data\n",
    "\n",
    "* Wikipedia\n",
    "\n",
    "* Code: GutHub (Microsoft), StackOverflow, BigQuery (Google), The Stack (HuggingFace)\n",
    "\n",
    "* Mixed data: The Pile, Dolma\n",
    "\n",
    "### Commonly Used Datasets for Fine-tuning\n",
    "\n",
    "* Instruction Tuning Datasets\n",
    "\n",
    "* Alignment Datasets\n",
    "\n",
    "### Library resources\n",
    "\n",
    "* Deep learning frameworks: PyTorch, TensorFlow, JAX, Keras, etc.\n",
    "\n",
    "* Transformers (HuggingFace)\n",
    "\n",
    "* DeepSpeed (Microsoft)\n",
    "\n",
    "* Megatron-LM (NVIDIA)\n",
    "\n",
    "* Others: Colossal-AI, BMTrain, FastMoE, vLLM, DeepSpeed-MII, DeepSpeed-Chat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training of PLMs\n",
    "\n",
    "#### Data collection and preparation\n",
    "\n",
    "* Data sources: General (web pages, conversation text, books), specialized (multilingual, scientific, code)\n",
    "\n",
    "#### Data preprocessing\n",
    "\n",
    "* Filtering and Selection: classifier-based (trained on high-quality data) and heuristic-based (lang id, perplexity, basic statistics, keyword filtering)\n",
    "\n",
    "* De-duplication: Remove repetitive patterns at word, sentence, paragraph and document levels\n",
    "\n",
    "* Privacy reduction\n",
    "\n",
    "* Tokenization: Byte-Pair Encoding, WordPiece and Unigram tokenizations.\n",
    "\n",
    "#### Data scheduling\n",
    "\n",
    "* Data mixture: proportion of each data source.\n",
    "\n",
    "* Different sources would be selected according to the mixture proportions\n",
    "\n",
    "* Data source heterogeneity is critical for improving the downstream performance\n",
    "\n",
    "* Data curriculum: order in which each data source is scheduled for training\n",
    "\n",
    "* Adaptive adjustment of data proportions for different sources during pre-training (easy samples first, then progressively introducing more challenging/specialized ones)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "Transformer is the common architecture behind PLMs\n",
    "\n",
    "#### Encoder-decoder\n",
    "\n",
    "Vanilla Transformer model with encoder and decoder stacks\n",
    "\n",
    "Mainly for text generation\n",
    "\n",
    "Examples: T5, BART, NLLB\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "Training based on masked tokens\n",
    "\n",
    "Mainly for classification task as document classification or sentiment analysis.\n",
    "\n",
    "Examples: BERT, ViT, etc.\n",
    "\n",
    "#### Causal decoder\n",
    "\n",
    "Unidirectional attention mask to guarantee that each input token can only attend to the past tokens and itself.\n",
    "\n",
    "Mainly for text generation\n",
    "\n",
    "Examples: GPT, LlaMa and Gemma families"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prefix (Non-casual) decoder\n",
    "\n",
    "Masking mechanism of causal decoders, to enable performing bidirectional attention over the prefix tokens and unidi-\n",
    "rectional attention only on generated tokens\n",
    "\n",
    "Examples: GLM-130B and U-PaLM\n",
    "\n",
    "#### Mixture-of-Experts\n",
    "\n",
    "A subset of neural network weights for each input are sparsely activated\n",
    "\n",
    "Examples: Switch Transformer and GLaM\n",
    "\n",
    "#### Emergent architectures\n",
    "\n",
    "Most based on parameterized state space models (SSM) \n",
    "\n",
    "Examples: Mamba, RetNet, RWKV and Hyena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "Detailed configuration: normalization, position embeddings, activation functions, and attention and bias\n",
    "\n",
    "#### Configuration \n",
    "\n",
    "<!-- https://www.reddit.com/r/MachineLearning/comments/t5dznr/r_deepnet_scaling_transformers_to_1000_layers/ -->\n",
    "* Normalization: LayerNorm (LN), RMSNorm, DeepNorm and Post-LN, Pre-LN, Sandwich-LN\n",
    "\n",
    "<!-- GLU paper: https://arxiv.org/pdf/1612.08083v3 -->\n",
    "* Activation functions: GeLU, variants of GLU (SwiGLEU, GeGLU)\n",
    "\n",
    "* Position Embeddings: Absolute, relative, rotatory (RoPE), ALiBi\n",
    "\n",
    "* Attention: Full (Vanilla Transformer), Sparse (local based on position), multi-query (key and value matrices shared across heads), grouped query (key and value matrices shared across group of heads), FlashAttention (optimization of GPU memory usage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective functions\n",
    "\n",
    "#### Language modeling (LM)\n",
    "\n",
    "Given a sequence of tokens $x_1^J$, a general training objective is to maximize the following log-likelihood:\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\cal L}(x) &= \\sum_{j=1}^J \\log p(x_i\\mid x_1^{j-1})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Denoising Autoencoding (DAE)\n",
    "\n",
    "Given a sequence of tokens $x$ in which a subset of them $\\tilde{X}$ have been corrupted in order to generate $\\tilde{x}$, a corrupted version of $x$, the objective function maximises the log-likelihood of the corrupted tokens:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\cal L}(x) &= \\sum_{u \\in \\tilde{X}} \\log p(u \\mid \\tilde{x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Mixture-of-Denoisers\n",
    "\n",
    "The loss function is computed according to the type of denoising that is applied to the sample. \n",
    "Samples are prefixed with the denoising type. \n",
    "Denoising varies in length (number of tokens involved) and ratio of corrupted text to generate:\n",
    "\n",
    "* S-denoiser (LM)\n",
    "* R-denoiser (DAE, short span and low corruption)\n",
    "* X-denoiser (DAE, long span or high corruption)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding strategies\n",
    "\n",
    "Greedy and beam search with length penalty\n",
    "\n",
    "$$\n",
    "\\hat{y}_1^{\\hat{I}} = \\operatorname*{argmax}_{y_1^I} P(y_1^I\\mid x_1^J)\n",
    "$$\n",
    "\n",
    "Random sampling: randomly select the next token based on the probability distribution to enhance the randomness and diversity during generation\n",
    "\n",
    "* Top-$k$ sampling: Randomly sample from $$ most probable tokens\n",
    "\n",
    "* Top-$p$ sampling: Randomly sample from those tokens accumulating a probability mass of $p$\n",
    "\n",
    "* Other strategies: $\\eta$-sampling, contrastive search and typical sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization settings\n",
    "\n",
    "Batch training: Large size (i.e. 4M tokens), dynamic schedule that increases batch size to stabilize training\n",
    "\n",
    "Learning rate: \n",
    "\n",
    "  * warm-up: initial 0.1% to 0.5% of the training steps, linear increasing up to  between $5\\cdot 10^{-5}$ and $1\\cdot 10^{-4}$\n",
    "\n",
    "  * decay strategy: cosine decrease to approximately 10% of its maximum value until the convergence\n",
    "\n",
    "Optimizer: Adam, AdamW, Adafactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-training\n",
    "\n",
    "### Instruction tuning\n",
    "\n",
    "Fine-tuning of pre-trained LLMs on a collection of formatted instances in the form of natural language\n",
    "\n",
    "After instruction tuning, LLMs demonstrate superior abilities to generalize to unseen\n",
    "tasks, even in a multilingual setting\n",
    "\n",
    "An instruction-formatted instance consists of a task description (called an instruction), an optional input,\n",
    "the corresponding output, and a small number of demonstrations (optional)\n",
    "\n",
    "Example\n",
    "\n",
    "*Instruction:* Please translate from English into Spanish:\n",
    "\n",
    "*Demonstration:* English: I'm fine Spanish: Estoy bien English: Good morning Spanish: Buenos días\n",
    "\n",
    "*Input/Output:* English: Are we there yet? Spanish: ¿Hemos llegado?\n",
    "\n",
    "Removing task description results in a dramatic drop in model performance\n",
    "\n",
    "Performance Improvement: smaller models with instruction tuning can even perform better\n",
    "than larger models without fine-tuning\n",
    "\n",
    "Instruction tuning is an effective approach to adapting existing general LLMs to be domain-specific experts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment tuning \n",
    "\n",
    "To align LLM output with human expectations or preferences avoiding unexpected behaviors (false information, pursuing inaccurate\n",
    "objectives, and producing harmful, misleading, and biased expressions)\n",
    "\n",
    "Alignment requires considering very different criteria (i.e., helpfulness, honesty, and harmlessness)\n",
    "\n",
    "It requires collection of human feedback: ranking of LLM outputs, answering questions with multiple options, degree of compliance with pre-defined rules\n",
    "\n",
    "Reinforcement learning from human feedback to fine-tune LLMs mostly based on the Proximal Pol-\n",
    "icy Optimization (PPO) algorithm\n",
    "\n",
    "### Parameter-Efficient Model Adaptation\n",
    "\n",
    "Motivation: full-parameter tuning is very costly\n",
    "\n",
    "* Adapter tuning\n",
    "\n",
    "* Prefix tuning\n",
    "\n",
    "* Prompt tuning\n",
    "\n",
    "* Low-Rank Adaptation (LoRA)\n",
    "\n",
    "Implemented in the library PEFT [3]: LoRA/AdaLoRA, prefix-tuning, P-Tuning, and prompt-tuning  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilization\n",
    "\n",
    "### Prompting\n",
    "\n",
    "A well-designed prompt is very helpful to elicit the abilities of LLMs for accomplishing specific tasks\n",
    "\n",
    "Components:\n",
    "\n",
    "- Task description\n",
    "\n",
    "- Input data\n",
    "\n",
    "- Contextual information\n",
    "\n",
    "- Prompt style\n",
    "\n",
    "\n",
    "Prompt optimization:\n",
    "\n",
    "- Discrete: Gradient-based, RL-based, Edit-based and LLM-based approaches\n",
    "\n",
    "- Continuous: Prompt learning with sufficient data and Prompt transferring with scarce data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Context Learning\n",
    "\n",
    "Formatted natural language prompt, consisting of the task description and/or a few task examples as demonstrations.\n",
    "\n",
    "The test instance is appended to the demonstration as the input for LLMs to generate the output\n",
    "\n",
    "Demonstration Design\n",
    "\n",
    "- Demonstration selection: Heuristic and LLM approaches\n",
    "\n",
    "- Demonstration Format\n",
    "\n",
    "- Demonstration Order\n",
    "\n",
    "ICL ability becomes more significant with a larger model size\n",
    "\n",
    "ICL by means of forward computation makes LLMs generate metagradients with respect to demonstrations and implicitly perform gradient descent via the attention mechanism\n",
    "\n",
    "### Chain-of-Thought Prompting\n",
    "\n",
    "Chain-of-Thought (CoT) prompting is an improved prompting strategy to boost the performance of LLMs on complex reasoning tasks\n",
    "\n",
    "CoT prompting further incorporates intermediate reasoning steps, which serve as the bridge between inputs and outputs\n",
    "\n",
    "### Planning\n",
    "\n",
    "Prompt-based planning has been proposed to break down complex tasks into smaller subtasks and generate a plan of actions to accomplish the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Europarl-ST\n",
    "\n",
    "<center>\n",
    "<table>\n",
    "<tr><td align=\"center\">English -></td><td colspan=\"2\" align=\"center\">Spanish</td><td colspan=\"2\" align=\"center\">German</td></tr>\n",
    "<tr><td>Model</td><td>BLEU</td><td>COMET</td><td>BLEU</td><td>COMET</td></tr>\n",
    "<tr><td>Llama3-8B</td><td>47.5</td><td>89.5</td><td>35.6</td><td>88.5</td></tr>\n",
    "<tr><td>Mistral-7B</td><td>46.8</td><td>89.5</td><td>34.5</td><td>88.4</td></tr>\n",
    "<tr><td>Llama2-7B</td><td>46.7</td><td>89.3</td><td>34.6</td><td>88.3</td></tr>\n",
    "<tr><td>Gemma-7B</td><td>46.6</td><td>89.2</td><td>34.5</td><td>88.2</td></tr>\n",
    "<tr><td>Falcon-7B</td><td>46.0</td><td>89.1</td><td>33.5</td><td>87.6</td></tr>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ol>\n",
    "<li><a href=\"https://arxiv.org/pdf/2303.18223\" target=\"_blank\">W. X. Zhao et al. A Survey of Large Language Models, arXiv preprint arXiv:2303.18223 (September 2024).</a></li>\n",
    "<li><a href=\"https://doi.org/10.1145/3649506\" target=\"_blank\">J. Yang et al. Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond, ACM Trans. Knowl. Discov. Data 18, 6, Article 160 (July 2024).</a></li>\n",
    "<li><a href=\"https://github.com/huggingface/peft\" target=\"_blank\"> S. Mangrulkar et al. The PEFT library.</li>\n",
    "<li><a href=\"https://aclanthology.org/2023.wmt-1.1.pdf\" target=\"_blank\">T. Kocmi et al. Findings of the 2023 Conference on Machine Translation (WMT23): LLMs Are Here But Not Quite There Yet, Proc. of WMT. Pages 1-42 (December 2023).</a></li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
