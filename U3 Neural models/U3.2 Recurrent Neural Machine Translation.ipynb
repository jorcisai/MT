{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Machine Translation\n",
    "\n",
    "From LM to MT: from monolingual to cross-lingual word prediction\n",
    "\n",
    "Early proposals with syncronized RNN [1]\n",
    "\n",
    "Non-syncronized RNN: Encoder-Decoder architecture [2]\n",
    "\n",
    "<img src=\"Sutskever2014_EncoderDecoder.svg\" width=\"1000\"/>\n",
    "\n",
    "A source LM (encoder) whose hidden state is transfered to a target LM (decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: node 'a1', graph '%3' size too small for label\n",
      "Warning: node 'e1', graph '%3' size too small for label\n",
      "Warning: node 'e2', graph '%3' size too small for label\n",
      "Warning: node 'a5', graph '%3' size too small for label\n",
      "Warning: node 'e6', graph '%3' size too small for label\n",
      "Warning: node 'a9', graph '%3' size too small for label\n",
      "Warning: node 'e10', graph '%3' size too small for label\n"
     ]
    }
   ],
   "source": [
    "import graphviz as G\n",
    "\n",
    "# boolean variables to denote dense or sparse connections between layers\n",
    "DENSE = True\n",
    "SPARSE = False\n",
    "\n",
    "\n",
    "TIMESTEPS = 11\n",
    "TIME_OFFSET = 3\n",
    "words=['&lt;s&gt;','the','house','is','blue','.','&lt;/s&gt;','la','casa','es','blanca','&lt;/s&gt;']\n",
    "\n",
    "unrolled = G.Digraph(node_attr={'shape':'circle', 'fixedsize':'true'}, graph_attr={'style':'invis', 'rankdir':'BT', 'color':'transparent'})\n",
    "\n",
    "i=0\n",
    "for step in range(TIMESTEPS+2):\n",
    "    if step == 0 or step == TIMESTEPS+1:\n",
    "        with unrolled.subgraph(name='cluster_'+str(i)) as c:\n",
    "            c.node('a'+str(step), '', color='transparent')\n",
    "            c.node('b'+str(step), '', color='transparent')\n",
    "            c.node('c'+str(step), '', color='transparent') \n",
    "            c.node('d'+str(step), '', color='transparent')\n",
    "            c.edge('a'+str(step), 'b'+str(step), style='invis') \n",
    "            c.edge('b'+str(step), 'c'+str(step), style='invis')\n",
    "            c.edge('c'+str(step), 'd'+str(step), style='invis')\n",
    "    else:\n",
    "        with unrolled.subgraph(name='cluster_'+str(i)) as c:\n",
    "            c.node('a'+str(step), words[TIMESTEPS-step], color='transparent');\n",
    "            c.node('b'+str(step), 'WE')\n",
    "            #c.node('c'+str(step), 't'+'{:=+d}'.format(TIME_OFFSET-step) if TIME_OFFSET-step else 't')\n",
    "            c.node('c'+str(step), '')\n",
    "            c.node('d'+str(step), 'SM');\n",
    "            #c.node('e'+str(step), '<w<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step+1)+'</sub>>' if TIME_OFFSET-step+1 else '<w<sub>'+'t'+'</sub>>', color='transparent');\n",
    "            c.node('e'+str(step), words[TIMESTEPS-step+1], color='transparent');\n",
    "            c.edge('a'+str(step), 'b'+str(step), label='<w<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step)+'</sub>>' if TIME_OFFSET-step else '<w<sub>'+'t'+'</sub>>'); \n",
    "            c.edge('b'+str(step), 'c'+str(step), label='<e<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step)+'</sub>>' if TIME_OFFSET-step else '<e<sub>'+'t'+'</sub>>'); \n",
    "            c.edge('c'+str(step), 'd'+str(step), label='<h<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step)+'</sub>>' if TIME_OFFSET-step else '<h<sub>'+'t'+'</sub>>');\n",
    "            c.edge('d'+str(step), 'e'+str(step), label='<y<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step+1)+'</sub>>' if TIME_OFFSET-step+1 else '<y<sub>'+'t'+'</sub>>');\n",
    "            \n",
    "for step in range(1, TIMESTEPS+1):\n",
    "    unrolled.edge('c'+str(step-1), 'c'+str(step), label='<s<sub>'+'t'+'{:=+d}'.format(TIME_OFFSET-step)+'</sub>>' if TIME_OFFSET-step else '<s<sub>'+'t'+'</sub>>', constraint='false', dir='back', color='black')\n",
    "\n",
    "unrolled.render(filename='RNNMT', format='svg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a source sentence $x_1^J$ search for a target sentence\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y}_1^{\\hat{I }} &= \\argmax_{y_1^I} P(y_1^I\\mid x_1^J)\\\\\n",
    "                     &= \\argmax_{y_1^I} \\prod_{i=1}^I p(y_i\\mid y_1^{i-1},x_1^J)\\\\\n",
    "                     &= \\argmax_{y_1^I} \\prod_{i=1}^I p(y_i\\mid y_1^{i-1},u(x_1^J))\n",
    "\\end{align}\n",
    "$$\n",
    "where $u(x_1^J)$ is a hidden-state representation of $x_1^J$.\n",
    "\n",
    "Search for the most likely translation using a simple left-to-right beam search decoder\n",
    "\n",
    "At each timestep each partial hypothesis (prefix translations) is extended with every word in the vocabulary.\n",
    "\n",
    "A small number $B$ of most-probable partial hypotheses are mantained\n",
    "\n",
    "When the symbol &lt;s&gt; appears in a hypothesis, it is considered to be complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results on WMT14 English-to-French [2]:\n",
    "\n",
    "|Systems|BLEU|\n",
    "|:------|---:|\n",
    "|Baseline (PB+Neural LM) | 33.3|\n",
    "|Ensemble of 5 LSTMs | 34.8|\n",
    "|Reescoring 1000-best with ensemble of 5 LSTMs | 36.5|\n",
    "|Moses for WMT14 | 37.0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with attention (Bahdanau et al.) [3]\n",
    "\n",
    "Conventional RNN *squash* all the source sentence information into a fixed-length vector\n",
    "\n",
    "RNN tend to to better represent recent inputs\n",
    "\n",
    "RNN with attention encode the input sentence into a sequence of vectors $h_1^J$ \n",
    "\n",
    "They select a subset of these vectors $h_1^J$ adaptively (attention mechanism) while decoding\n",
    "$$\n",
    "\\hat{y}_1^{\\hat{I }} = \\argmax_{y_1^I} \\prod_{i=1}^I p(y_i\\mid y_1^{i-1},u_i)\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\begin{align}\n",
    "u_i &= a(h^e_1, \\cdots, h^e_J, h^d_{i-1})\\\\ \n",
    "    &= \\sum_{j=1}^{J} \\alpha(h^e_1, \\cdots, h^e_J, h^d_{i-1})\\,h^e_j\n",
    "\\end{align}    \n",
    "$$\n",
    "being $\\alpha(h^e_j, h^d_{i-1})$ a similarity measure (alignment) between the encoder state at position $j$ and \n",
    "the decoder state at position $i-1$.\n",
    "\n",
    "Bahdanau et al. define this similarity measure as a probability distribution over source positions\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha(h^e_1, \\cdots, h^e_J, h^d_{i-1}) &= \\alpha_j(h^e_1, \\cdots, h^e_J, h^d_{i-1})\\\\\n",
    "                         &= \\alpha_{ij}\\\\\n",
    "                         &= \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{J} \\exp(e_{ik})}\n",
    "\\end{align}\n",
    "$$\n",
    "being\n",
    "$$\n",
    "e_{ij} = v_a^t \\tanh \\left( W_a h^d_{i-1} + U_a h^e_j \\right)\n",
    "$$\n",
    "where $v_a \\in \\mathbb{R}$, $W_a \\in \\mathbb{R}^{n \\times n}$ and $U_a \\in \\mathbb{R}^{n \\times 2n}$\n",
    "\n",
    "Consequently, the hidden state of the RNN decoder $h^d_i$ is enriched with $u_i$ as\n",
    "$$\n",
    "h^d_i = F(h^d_{i-1}, y_{i−1}, u_i)\n",
    "$$\n",
    "and the output layer computing the probability over the vocabulary also incorporates $u_i$ and the residual connection $y_{i-1}$ \n",
    "$$\n",
    "p(y_i\\mid y_1^{i-1},u_i) = g(h^d_i, y_{i−1}, u_i)\n",
    "$$\n",
    "Indeed, the RNN hidden state at the encoder $h_j$ is the concatenation of the hidden state of a forward and a backward RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bahdanau2015_EncoderDecoder.svg\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with attention (Luong et al.) [4]\n",
    "\n",
    "Simplification and generalisation over [3]\n",
    "\n",
    "  * Unidirectional encoder\n",
    "  * Alternative attention mechanism: $ e_{ij} = \\left\\{ \\begin{matrix} \\left(h_i^d\\right)^t \\, h^e_j\\\\\\left(h_i^d\\right)^t \\, W_a \\, h^e_j\\\\ W_a \\, [h^d_i;h^e_j]  \\end{matrix} \\right.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Luong2015_EncoderDecoder.svg\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, in 2014, NMT systems still behind SOTA phrase-based systems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Is All You Need [5]\n",
    "\n",
    "Additional parameters $W_V$, $W_Q$ and $W_K$ are incorporated in the attention mechanism\n",
    "\n",
    "$$\n",
    "u_i = \\sum_{j=1}^{J} \\alpha(h^e_1, \\cdots, h^e_J, h^d_{i-1})\\,W_V\\,h^e_j\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\alpha(h^e_1, \\cdots, h^e_J, h^d_{i-1}) = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{J} \\exp(e_{ik})}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "e_{ij} = W_Q\\,h_{i-1}^d \\cdot W_K\\,h_j^e\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, there is much more\n",
    "\n",
    "  * RNN replaced by FFN: direct connection between source and target words and faster computation\n",
    "  * Multi-layer self-attention and cross-attention\n",
    "$$\n",
    "\\hat{y}_1^{\\hat{I }} = \\argmax_{y_1^I} \\prod_{i=1}^I p(y_i\\mid v(u(y_1^{i-1}),u(x_1^J)))\n",
    "$$  \n",
    "  * Multi-head attention \n",
    "  * Positional encoding \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer self-attention and cross-attention\n",
    "\n",
    "The original Transformer is a 6-layer encoder with self-attention plus 6-layer self-attention+cross-attention decoder.\n",
    "\n",
    "\n",
    "## Encoder\n",
    "\n",
    "Each layer of the encoder can be depicted as follows\n",
    "\n",
    "<img src=\"Encoder_Transformer.svg\" width=\"500\"/>\n",
    "\n",
    "In the layer $l$ of the encoder\n",
    "\n",
    "$$u^{e,l}_j = \\sum_{j=1}^{J} \\alpha(h^{e,l-1}_1, \\cdots, h^{e,l-1}_J, h^{e,l-1}_j)\\,W^{e,l}_V\\,h^{e,l-1}_j$$\n",
    "\n",
    "and a feed-forward net to generate the hidden state of layer $l$ at the encoder\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h^{e,l}_j &= F(u^{e,l}_j)\\\\\n",
    "          &= W_2^{e,l}\\,\\operatorname{ReLU}\\left(W_1^{e,l}\\,u_j^{e,l}+b_1^{e,l}\\right)+b_2^{e,l}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $W_2^{e,l}$, $W_1^{e,l}$ are the weights of the second and first layer of the\n",
    "feed-forward networks, and $b_2^{e,l}$ and $b_1^{e,l}$ the corresponding bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Decoder\n",
    "\n",
    "In the case of the decoder, layer l is represented as\n",
    "\n",
    "<img src=\"Decoder_Transformer.svg\" width=\"500\"/>\n",
    "\n",
    "In the layer $l$ of the decoder, first, self-attention similar to the encoder self-attention but limited to the first i positions\n",
    "\n",
    "$$u^{d,l}_i = \\sum_{i=1}^{i} \\alpha(h^{d,l-1}_1, \\cdots, h^{d,l-1}_i, h^{d,l-1}_i)\\,W^{d,l}_V\\,h^{d,l-1}_i$$\n",
    "\n",
    "Followed by cross-attention to all source positions of the last encoder layer\n",
    "\n",
    "$$v^{d,l}_i = \\sum_{j=1}^{J} \\alpha(h^{e,L}_1, \\cdots, h^{e,L}_J, u^{d,l}_i)\\,W^{c,l}_V\\,h^{e,L}_j$$\n",
    "\n",
    "Finally, similarly to the encoder, the feed-forward net to generate the hidden state of layer $l$ at the decoder\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h^{d,l}_i &= F(v^{d,l}_i)\\\\\n",
    "          &= W_2^{d,l}\\,\\operatorname{ReLU}\\left(W_1^{d,l}\\,v_i^{d,l}+b_1^{d,l}\\right)+b_2^{d,l}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head attention\n",
    "\n",
    "At each layer, N indepedent attention mechanism $\\left( W_Q,\\,W_K,\\,W_V \\right)$ to jointly attend to information from different representation\n",
    "subspaces at different positions\n",
    "\n",
    "Applied to self-attention encoder, self-attention decoder and cross-attention decoder\n",
    "\n",
    "For instance, self-attention encoder for the $n$-th head\n",
    "\n",
    "$$u^{e,l,n}_j = \\sum_{j=1}^{J} \\alpha(h^{e,l-1}_1, \\cdots, h^{e,l-1}_J, h^{e,l-1}_j;\\,W^{e,l,n}_Q,\\,W^{e,l,n}_K\\,)\\,W^{e,l,n}_V\\,h^{e,l-1}_j$$\n",
    "\n",
    "The $N$ representations of $u^{e,l}_j$ are concatenated and projected via a feed-forward net\n",
    "\n",
    "$$\n",
    "u^{e,l}_j = F\\left(\\left[u^{e,l,n}_1;\\cdots;u^{e,l,n}_J\\right]\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ol>\n",
    "<li><a href=\"https://www.isca-archive.org/eurospeech_1997/castano97_eurospeech.pdf\" target=\"_blank\">M.A. Castaño and F. Casacuberta. A Connectionist Approach to Machine Translation, EuroSpeech 1997.</a></li>\n",
    "<li><a href=\"https://papers.nips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf\" target=\"_blank\">I. Sutskever et al. Sequence to Sequence Learning with Neural Networks, NIPS 2014.</a></li>\n",
    "<li><a href=\"https://arxiv.org/pdf/1409.0473\" target=\"_blank\">D. Bahdanau et al. Neural Machine Translation by Jointly Learning to Align and Translate, ICLR 2015.</a></li>\n",
    "<li><a href=\"https://aclanthology.org/D15-1166.pdf\" target=\"_blank\">M. Luong et al. Effective Approaches to Attention-based Neural Machine Translation, EMNLP 2015.</a></li>\n",
    "<li><a href=\"https://arxiv.org/pdf/1706.03762\" target=\"_blank\">A. Vaswani et al. Attention Is All You Need, NIPS 2017.</a></li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tfcuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
