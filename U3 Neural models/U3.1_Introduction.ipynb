{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Machine Translation</h1>\n",
    "</center>\n",
    "\n",
    "<br><br><br><br><br><br><br><br>\n",
    "\n",
    "<center>\n",
    "<h1>Neural models</h1>\n",
    "</center>\n",
    "\n",
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework\n",
    "\n",
    "Neural MT is based on the modelisation of  $P(y \\mid x)$ with a neural network (NN) to perform\n",
    "\n",
    "$$\\hat{y} = \\operatorname*{argmax}_{y} P(y \\mid x)$$\n",
    "\n",
    "where $x$ is a source sentence and $y$ is a target sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminars\n",
    "\n",
    "Processing a sequence of words by a NN is a challenging task because of its discrete nature \n",
    "\n",
    "Previous works had to work with very limited vocabulary (a few tens of words):\n",
    "  * Language modelling with Elman (recurrent) NN [1]\n",
    "  * Machine translation with RNN [2]\n",
    "\n",
    "Need of reducing the vocabulary size to reduce computational requirements\n",
    "  * Tokeniser: language dependent and not reversibly convertible\n",
    "  * Unsupervised reversible tokenisers: Byte-Pair Encoding [7], SentencePiece [8], etc.\n",
    "\n",
    "Need of mapping word representation to a continuous space to be processed by a NN\n",
    "  * Unsuccessful attempts by using bag-of-words, latent semantic indexing, word classes, clustering, etc.\n",
    "  * Learning a feature vector for each word in context together with the task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised reversible tokenisers\n",
    "\n",
    "### Byte-pair Encoding (BPE)\n",
    "  * It iteratively merges the most frequent pair of symbols up to a maximum number of merge operations\n",
    "  * It starts with a symbol vocabulary that is the character vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n",
      "lowest\n",
      "lower\n",
      "new\n",
      "newest\n",
      "newer\n",
      "wider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|##########################################| 3/3 [00:00<00:00, 12180.94it/s]\n",
      "#version: 0.2\n",
      "w e\n",
      "n e\n",
      "l o\n",
      "-----------\n",
      "lo@@ w\n",
      "lo@@ we@@ s@@ t\n",
      "lo@@ we@@ r\n",
      "ne@@ w\n",
      "ne@@ we@@ s@@ t\n",
      "ne@@ we@@ r\n",
      "w@@ i@@ d@@ e@@ r\n"
     ]
    }
   ],
   "source": [
    "!echo -e  \"low\\nlowest\\nlower\\nnew\\nnewest\\nnewer\\nwider\" | tee train.txt\n",
    "!subword-nmt learn-bpe --min-frequency 1 -s 3 < train.txt > codes.txt\n",
    "!cat codes.txt; echo \"-----------\"\n",
    "!subword-nmt apply-bpe -c codes.txt < train.txt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised reversible tokenisers\n",
    "\n",
    "### SentencePiece\n",
    "  * Integrates the implementation of the unigram (*uni-subword*) language model [9] \n",
    "  * Capable of outputing multiple subword segmentations with probabilities\n",
    "  * The most probable subword segmentations can be considered for training and decoding\n",
    "  * No need of previous tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "params = ('--input=train.txt ''--model_prefix=train ''--vocab_size=18')\n",
    "spm.SentencePieceTrainer.Train(params)\n",
    "sp = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\t0\n",
      "<s>\t0\n",
      "</s>\t0\n",
      "r\t-1.94426\n",
      "s\t-2.44426\n",
      "t\t-2.44426\n",
      "▁lowe\t-2.47125\n",
      "▁newe\t-2.47126\n",
      "e\t-3.31599\n",
      "▁new\t-3.37828\n",
      "▁low\t-3.37829\n",
      "o\t-3.44406\n",
      "n\t-3.44416\n",
      "d\t-3.44426\n",
      "i\t-3.44426\n",
      "l\t-3.44426\n",
      "w\t-3.44426\n",
      "▁\t-3.44426\n"
     ]
    }
   ],
   "source": [
    "!cat train.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁lowe', 's', 't'],\n",
       " ['▁low', 'e', 's', 't'],\n",
       " ['▁', 'l', 'o', 'w', 'e', 's', 't']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.Load('train.model')\n",
    "sp.nbest_encode('lowest', nbest_size=3, out_type=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['▁lowe', 's', 't'], -0.003965185023844242),\n",
       " (['▁low', 'e', 's', 't'], -0.026968346908688545)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.sample_encode_and_score('lowest', num_samples=2, alpha=0.1, out_type=str, wor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning a word feature vector, a.k.a. word embedding, jointly with the language modelling task [3]\n",
    "  * Feedforward NN achieved 20% perplexity relative reduction w.r.t. n-grams\n",
    "  * Maximise log-likelihood = minimise cross-entropy = minimise perplexity\n",
    "  * Still limited vocabulary size (tens of thousands) and running words (a few millions)\n",
    "  * A few tens of hidden units -> training time: one week per epoch in 40 CPUs \n",
    "\n",
    "<img src=\"MLPLM.svg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modelling on real-tasks with RNN [4]\n",
    "  * From backpropagation to backpropagation through time\n",
    "  * Various optimisations to scale up running words in training (hundreds of millions)\n",
    "  * Numerical stability issues: double precision and gradient explosion (truncation)\n",
    "  * Relative reduction of 20% in state-of-the-art ASR tasks\n",
    "  \n",
    "  <img src=\"RNNLM.svg\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of RNN\n",
    "\n",
    "Output depens not only on near but also far context in a sentence\n",
    "\n",
    "Numerical instability issues with gradients (propagated error) vanishing and exploiding\n",
    "\n",
    "Solution: Long Short-Term Memory (LSTM) [5] or Gated Recurrent Units (GRU) [6] cells\n",
    "\n",
    "## LSTM cell\n",
    "\n",
    "It replaces the nodes in the hidden layer\n",
    "\n",
    "It explicitly models a memory state to retain near/far context\n",
    "\n",
    "Output and memory state change depends on parametrised *gates*:\n",
    "  * input gate: how much new input changes memory state\n",
    "  * forget gate: how much of prior memory state is retained\n",
    "  * output gate: how strongly memory state is passed on to next layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ol>\n",
    "<li><a href=\"https://onlinelibrary.wiley.com/doi/epdf/10.1207/s15516709cog1402_1\" target=\"_blank\">J. Elman. Finding Structure in Time, Cognitive Science 1990.</a></li>\n",
    "<li><a href=\"https://www.isca-archive.org/eurospeech_1997/castano97_eurospeech.pdf\" target=\"_blank\">M.A. Castaño and F. Casacuberta. A Connectionist Approach to Machine Translation, EuroSpeech 1997.</a></li>\n",
    "<li><a href=\"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\" target=\"_blank\">Y. Bengio et al. A Neural Probabilistic Language Model, Journal of Machine Learning Research 2003.</a></li>\n",
    "<li><a href=\"https://www.fit.vut.cz/study/phd-thesis-file/283/283.pdf\" target=\"_blank\">T. Mikolov. Statistical Language Models based on Neural Networks, Ph.D. Thesis 2012.</a></li>\n",
    "<li><a href=\"https://www.bioinf.jku.at/publications/older/2604.pdf\" target=\"_blank\">S. Hochreiter and J. Schmidhuber. Long short-term memory, Neural Computation 1997.</a></li>\n",
    "<li><a href=\"https://arxiv.org/pdf/1406.1078\" target=\"_blank\">K. Cho et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, ACL 2014.</a></li>\n",
    "<li><a href=\"https://aclanthology.org/P16-1162.pdf\" target=\"_blank\">R. Sennrich et al. Neural Machine Translation of Rare Words with Subword Units, ACL 2016.</a></li>\n",
    "<li><a href=\"https://aclanthology.org/D18-2012.pdf\" target=\"_blank\">T. Kudo and J. Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing, EMNLP 2018.</a></li>\n",
    "<li><a href=\"https://aclanthology.org/P18-1007.pdf\" target=\"_blank\">T. Kudo. Subword Regularization: Improving Neural Network Translation Models\n",
    "with Multiple Subword Candidates, ACL 2018.</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tfcuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
