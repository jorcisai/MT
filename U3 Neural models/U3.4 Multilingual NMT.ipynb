{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual NMT [1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Low-resource languages\n",
    "\n",
    "No-resource language: zero-shot translation models\n",
    "\n",
    "To take advantage of common features in similar languages\n",
    "\n",
    "A single engine instead many engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible settings\n",
    "\n",
    "Multi-way translation. The goal is constructing a single NMT system\n",
    "for one-to-many, many-to-one or many-to-many translation using parallel\n",
    "corpora for more than one language pair. Parallel corpora are available\n",
    "for each language pairs\n",
    "\n",
    "Low resource translation. (a) a high-resource language pair is available\n",
    "to assist a low-resource language pair. (b) no direct parallel corpus for\n",
    "the low-resource pair and a pivot language is used.\n",
    "\n",
    "\n",
    "Multi-source translation. Documents that have to be translated into more\n",
    "than one language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-way NMT\n",
    "\n",
    "One encoder for all source languages or one encoder for\n",
    "each source language\n",
    "\n",
    "One decoder for all target languages or one decoder for\n",
    "each target language\n",
    "\n",
    "### From minimal to complete parameter sharing\n",
    "\n",
    "One encoder and one decoder for each language: the burden is on the shared attention layer\n",
    "\n",
    "One encoder for all: common vocabulary across all languages (BPE, SentencePiece, etc.)\n",
    "\n",
    "One decoder for all: \n",
    "  * prefix sentences with language tag\n",
    "  * vocabulary increase: larger softmax layer and slower inference\n",
    "  * particularly useful for related languages\n",
    "\n",
    "Complete parameter sharing:\n",
    "  * adopted by massively multilingual NMT\n",
    "  * suffer from representation bottlenecks <!-- not all translation directions show improved performance despite a massive amount of data\n",
    "being fed to a model with a massive number of parameters -->\n",
    "\n",
    "Controlled Parameter Sharing\n",
    "  * degree of parameter sharing is the divergence between the languages involved\n",
    "  * simplicity\n",
    "  * flexibility of modelling\n",
    "  <!-- target language-specific attention performs better than other attention-sharing configurations -->\n",
    "  * learning the degree of parameter sharing from the training data\n",
    "\n",
    "\n",
    "### Addressing Language Divergence\n",
    "\n",
    "Select vocabulary from different languages and learn sub-word vocabulary to avoid OOV words\n",
    "\n",
    "Representation similarity varies across layers: lower variance at inner layers of encoder/decoder\n",
    "\n",
    "Source sentence length impacts encoder representation \n",
    "\n",
    "Language tag proved to be very effective in decoder representation\n",
    "\n",
    "### Training protocols\n",
    "\n",
    "Fully shared models, a single batch ccontain sentence pairs from multiple language pairs\n",
    "\n",
    "Oversample smaller datasets to match the sizes of the largest datasets\n",
    "\n",
    "Knowledge Distillation: train a large model with many layers and then distill its knowledge into a smaller model\n",
    "\n",
    "Going beyong oversampling for low-resource languages\n",
    "\n",
    "## Low-resource languages\n",
    "\n",
    "Data augmentation strategies can improve translation quality: back-translation and self-training\n",
    "\n",
    "\n",
    "Training rich-resource (parent) and low-resource languages:\n",
    "\n",
    "  * Jointly training when sharing the same target language\n",
    "\n",
    "  * Fine-tune: Train parent model and fine-tune it on the low-resource pair\n",
    "\n",
    "  * Transfer learning easier on the source-side than on the target-side\n",
    "    <!--  need of specific target language representation -->\n",
    "  * A related parent language benefits the child language more than an unrelated parent\n",
    "\n",
    "## MNMT for unseen languages pairs\n",
    "\n",
    "Can we do better than unsupervised NMT by utilizing multilingual translation corpora?\n",
    "\n",
    "### Pivot translation\n",
    "\n",
    "Even if two languages do not have a parallel corpus, they are likely to share a\n",
    "parallel corpus with a third language (called the pivot language)\n",
    "\n",
    "Cascaded approach: independent source-pivot (S-P) and pivot-target (P-T) MT systems\n",
    "\n",
    "Limitations: error propagation and double decoding time\n",
    "\n",
    "Re-ranking of n-best translations can improve performance\n",
    "\n",
    "### Zero-shot Translation\n",
    "\n",
    "The MNMT system has not been trained for the unseen language pair...\n",
    "\n",
    "... but the system is able to generate reasonable target language translations for the source sentence\n",
    "\n",
    "Need of target language tag as input to generate desired target language\n",
    "\n",
    "Translation between any arbitrary language pair, while source and target seeing in training\n",
    "\n",
    "Its performance is generally lower than the pivot translation system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-resource Translation\n",
    "\n",
    "Optimizing the translation quality of a specific unseen pair when no resources available for that pair\n",
    "\n",
    "Synthetic Corpus Generation: \n",
    "\n",
    "  * Pivot sentences of a P-T corpora are backtranslated to the source language to create S-T corpora\n",
    "  * Cascade system S-P and P-T\n",
    "\n",
    "Combining Pre-trained Encoders and Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ol>\n",
    "<li><a href=\"https://dl.acm.org/doi/pdf/10.1145/3406095\" target=\"_blank\">R. Dabre et al. A Survey of Multilingual Neural Machine Translation, ACM Computing Survey 2020.</a></li>\n",
    "<li><a href=\"https://arxiv.org/pdf/2404.04925\" target=\"_blank\">L. Qin et al. Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers, arXiv 2024.</a></li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
