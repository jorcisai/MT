{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Large Models (PLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Training neural machine translation models from scratch is expensive\n",
    "\n",
    "Large bilingual corpora are necessary? \n",
    "\n",
    "For many task-specific translation models there are low resources\n",
    "\n",
    "Pre-trained large models have proved to be useful in many scenarios, but in MT?\n",
    "\n",
    "  * Combination of pre-trained encoders and pre-trained decoders?\n",
    "  * Multilingual pretrained language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "PLMs usually refer to Transformer-based architectural models with billions of parameters and trained on massive data\n",
    "\n",
    "PLMs exhibit strong capacities to understand natural language and solve complex tasks such as machine translation\n",
    "\n",
    "### Scaling laws\n",
    "\n",
    "Model performance/capacity as a function of model and data size, and computational budget\n",
    "\n",
    "Given a limited compute budget, KM scaling laws favors a larger budget allocation in model size than the data\n",
    "size, while the Chinchilla scaling law argues that the two sizes should be increased in equal scales\n",
    "\n",
    "* Predictable scaling: loss decrease on smaller models convey to larger models\n",
    "* Task-level scaling: loss decrease does not always implies better performance in downstream tasks\n",
    "\n",
    "### Emergent abilities\n",
    "\n",
    "Abilities that are not present in small models but arise in large models\n",
    "\n",
    "* In-context learning. The LM provided with a natural language instruction and/or several task demonstrations, it can generate the expected output for the test instances by completing the word sequence of input text, without requiring additional training or gradient update\n",
    "\n",
    "* Instruction following. By fine-tuning with a mixture of multi-task datasets formatted via natural language descrip-\n",
    "tions without using explicit examples, LLMs are shown to perform well on unseen tasks that are also described in the form of instructions. \n",
    "<!--  According to the experiments in [67], instruction-tuned LaMDA-PT [68] started to significantly\n",
    "outperform the untuned one on unseen tasks when the model size reached 68B, but not for 8B or smaller model sizes. A recent study [69] found that a model size of 62B is at least required for PaLM to perform well on various tasks in four evaluation benchmarks (i.e., MMLU, BBH, TyDiQA and MGSM), though a much smaller size might suffice for some specific tasks (e.g., MMLU). >\n",
    "\n",
    "* Chain-of-thought (CoT): Step-by-step reasoning to solve complex tasks such mathematical word problem.\n",
    "<!-- CoT prompting can bring performance gains (on arithmetic reasoning benchmarks) when applied to PaLM and LaMDA variants with a model size larger than 60B, while its advantage over the standard prompting becomes more evident when the model size exceeds 100B >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources of PLMs\n",
    "\n",
    "### Publicly Available Model Checkpoints or APIs\n",
    "\n",
    "LlaMa family (Meta): Latest version is LlaMa 3.1\n",
    "\n",
    "* Largest version has 405B parameters, 15T training tokens, and an extended context\n",
    "window of 128K tokens \n",
    "* It achieves competitive performance against prominent closed-source LLMs, such as GPT-4, in various benchmark\n",
    "* It has greatly advanced the research progress of LLMs\n",
    "\n",
    "Mistral family: Mistral Large 2 (123B parameters)\n",
    "\n",
    "* Strong performance on various mainstream benchmarks (e.g., MMLU and GSM8k)\n",
    "\n",
    "Gemma family (Google): Gemma\n",
    "\n",
    "* Achieved excellent performance in multiple benchmarks (e.g., ARC-c, MMLU, and GSM8k)\n",
    "\n",
    "Other families: Qwen, GLM, Baichuan. etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commonly Used Corpora for Pre-training\n",
    "\n",
    "* Web pages: CommonCrawl, C4, RedPajama-Data, RefinedWeb, WebText\n",
    "\n",
    "* Books & Academic Data: Book Data, Academic data\n",
    "\n",
    "* Wikipedia\n",
    "\n",
    "* Code: GutHub (Microsoft), StackOverflow, BigQuery (Google), The Stack (HuggingFace)\n",
    "\n",
    "* Mixed data: The Pile, Dolma\n",
    "\n",
    "### Commonly Used Datasets for Fine-tuning\n",
    "\n",
    "* Instruction Tuning Datasets\n",
    "\n",
    "* Alignment Datasets\n",
    "\n",
    "### Library resources\n",
    "\n",
    "* Deep learning frameworks: PyTorch, TensorFlow, JAX, Keras, etc.\n",
    "\n",
    "* Transformers (HuggingFace)\n",
    "\n",
    "* DeepSpeed (Microsoft)\n",
    "\n",
    "* Megatron-LM (NVIDIA)\n",
    "\n",
    "* Others: Colossal-AI, BMTrain, FastMoE, vLLM, DeepSpeed-MII, DeepSpeed-Chat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training of PLMs\n",
    "\n",
    "#### Data collection and preparation\n",
    "\n",
    "* Data sources: General (web pages, conversation text, books), specialized (multilingual, scientific, code)\n",
    "\n",
    "#### Data preprocessing\n",
    "\n",
    "* Filtering and Selection: classifier-based (trained on high-quality data) and heuristic-based (lang id, perplexity, basic statistics, keyword filtering)\n",
    "\n",
    "* De-duplication: Remove repetitive patterns at word, sentence, paragraph and document levels\n",
    "\n",
    "* Privacy reduction\n",
    "\n",
    "* Tokenization: Byte-Pair Encoding, WordPiece and Unigram tokenizations.\n",
    "\n",
    "#### Data scheduling\n",
    "\n",
    "* Data mixture: proportion of each data source.\n",
    "\n",
    "* Different sources would be selected according to the mixture proportions\n",
    "\n",
    "* Data source heterogeneity is critical for improving the downstream performance\n",
    "\n",
    "* Data curriculum: order in which each data source is scheduled for training\n",
    "\n",
    "* Adaptive adjustment of data proportions for different sources during pre-training (easy samples first, then progressively introducing more challenging/specialized ones)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "Transformer is the common architecture behind PLMs\n",
    "\n",
    "#### Encoder-decoder\n",
    "\n",
    "Vanilla Transformer model with encoder and decoder stacks\n",
    "\n",
    "Mainly for text generation\n",
    "\n",
    "Examples: T5, BART, NLLB\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "Training based on masked tokens\n",
    "\n",
    "Mainly for classification task as document classification or sentiment analysis.\n",
    "\n",
    "Examples: BERT, ViT, etc.\n",
    "\n",
    "#### Causal decoder\n",
    "\n",
    "Unidirectional attention mask to guarantee that each input token can only attend to the past tokens and itself.\n",
    "\n",
    "Mainly for text generation\n",
    "\n",
    "Examples: GPT, LlaMa and Gemma families\n",
    "\n",
    "#### Prefix (Non-casual) decoder\n",
    "\n",
    "Masking mechanism of causal decoders, to enable performing bidirectional attention over the prefix tokens and unidi-\n",
    "rectional attention only on generated tokens\n",
    "\n",
    "Examples: GLM-130B and U-PaLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of pre-trained large models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"EvolutionaryTreeLLMs.jpg\" width=\"640\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ol>\n",
    "<li><a href=\"https://arxiv.org/pdf/2303.18223\" target=\"_blank\">W. X. Zhao et al. A Survey of Large Language Models, arXiv preprint arXiv:2303.18223 (September 2024).</a></li>\n",
    "<li><a href=\"https://doi.org/10.1145/3649506\" target=\"_blank\">J. Yang et al. Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond, ACM Trans. Knowl. Discov. Data 18, 6, Article 160 (July 2024).</a></li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
