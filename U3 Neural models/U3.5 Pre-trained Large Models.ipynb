{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Large Models (PLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Training neural machine translation models from scratch is expensive\n",
    "\n",
    "Large bilingual corpora are necessary? \n",
    "\n",
    "For many task-specific translation models there are low resources\n",
    "\n",
    "Pre-trained large models have proved to be useful in many scenarios, but in MT?\n",
    "\n",
    "  * Combination of pre-trained encoders and pre-trained decoders?\n",
    "  * Multilingual pretrained language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "PLMs usually refer to Transformer-based architectural models with billions of parameters and trained on massive data\n",
    "\n",
    "PLMs exhibit strong capacities to understand natural language and solve complex tasks such as machine translation\n",
    "\n",
    "### Scaling laws\n",
    "\n",
    "Model performance/capacity as a function of model and data size, and computational budget\n",
    "\n",
    "Given a limited compute budget, KM scaling laws favors a larger budget allocation in model size than the data\n",
    "size, while the Chinchilla scaling law argues that the two sizes should be increased in equal scales\n",
    "\n",
    "* Predictable scaling: loss decrease on smaller models convey to larger models\n",
    "* Task-level scaling: loss decrease does not always implies better performance in downstream tasks\n",
    "\n",
    "### Emergent abilities\n",
    "\n",
    "Abilities that are not present in small models but arise in large models\n",
    "\n",
    "* In-context learning. The LM provided with a natural language instruction and/or several task demonstrations, it can generate the expected output for the test instances by completing the word sequence of input text, without requiring additional training or gradient update\n",
    "\n",
    "* Instruction following. By fine-tuning with a mixture of multi-task datasets formatted via natural language descrip-\n",
    "tions without using explicit examples, LLMs are shown to perform well on unseen tasks that are also described in the form of instructions. \n",
    "<!--  According to the experiments in [67], instruction-tuned LaMDA-PT [68] started to significantly\n",
    "outperform the untuned one on unseen tasks when the model size reached 68B, but not for 8B or smaller model sizes. A recent study [69] found that a model size of 62B is at least required for PaLM to perform well on various tasks in four evaluation benchmarks (i.e., MMLU, BBH, TyDiQA and MGSM), though a much smaller size might suffice for some specific tasks (e.g., MMLU). >\n",
    "\n",
    "* Chain-of-thought (CoT): Step-by-step reasoning to solve complex tasks such mathematical word problem.\n",
    "<!-- CoT prompting can bring performance gains (on arithmetic reasoning benchmarks) when applied to PaLM and LaMDA variants with a model size larger than 60B, while its advantage over the standard prompting becomes more evident when the model size exceeds 100B >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources of PLMs\n",
    "\n",
    "### Publicly Available Model Checkpoints or APIs\n",
    "\n",
    "LlaMa family (Meta): Latest version is LlaMa 3.1\n",
    "\n",
    "* Largest version has 405B parameters, 15T training tokens, and an extended context\n",
    "window of 128K tokens \n",
    "* It achieves competitive performance against prominent closed-source LLMs, such as GPT-4, in various benchmark\n",
    "* It has greatly advanced the research progress of LLMs\n",
    "\n",
    "Mistral family: Mistral Large 2 (123B parameters)\n",
    "\n",
    "* Strong performance on various mainstream benchmarks (e.g., MMLU and GSM8k)\n",
    "\n",
    "Gemma family (Google): Gemma\n",
    "\n",
    "* Achieved excellent performance in multiple benchmarks (e.g., ARC-c, MMLU, and GSM8k)\n",
    "\n",
    "Other families: Qwen, GLM, Baichuan. etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commonly Used Corpora for Pre-training\n",
    "\n",
    "* Web pages: CommonCrawl, C4, RedPajama-Data, RefinedWeb, WebText\n",
    "\n",
    "* Books & Academic Data: Book Data, Academic data\n",
    "\n",
    "* Wikipedia\n",
    "\n",
    "* Code: GutHub (Microsoft), StackOverflow, BigQuery (Google), The Stack (HuggingFace)\n",
    "\n",
    "* Mixed data: The Pile, Dolma\n",
    "\n",
    "### Commonly Used Datasets for Fine-tuning\n",
    "\n",
    "* Instruction Tuning Datasets\n",
    "\n",
    "* Alignment Datasets\n",
    "\n",
    "### Library resources\n",
    "\n",
    "* Deep learning frameworks: PyTorch, TensorFlow, JAX, Keras, etc.\n",
    "\n",
    "* Transformers (HuggingFace)\n",
    "\n",
    "* DeepSpeed (Microsoft)\n",
    "\n",
    "* Megatron-LM (NVIDIA)\n",
    "\n",
    "* Others: Colossal-AI, BMTrain, FastMoE, vLLM, DeepSpeed-MII, DeepSpeed-Chat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training of PLMs\n",
    "\n",
    "#### Data collection and preparation\n",
    "\n",
    "* Data sources: General (web pages, conversation text, books), specialized (multilingual, scientific, code)\n",
    "\n",
    "#### Data preprocessing\n",
    "\n",
    "* Filtering and Selection: classifier-based (trained on high-quality data) and heuristic-based (lang id, perplexity, basic statistics, keyword filtering)\n",
    "\n",
    "* De-duplication: Remove repetitive patterns at word, sentence, paragraph and document levels\n",
    "\n",
    "* Privacy reduction\n",
    "\n",
    "* Tokenization: Byte-Pair Encoding, WordPiece and Unigram tokenizations.\n",
    "\n",
    "#### Data scheduling\n",
    "\n",
    "* Data mixture: proportion of each data source.\n",
    "\n",
    "* Different sources would be selected according to the mixture proportions\n",
    "\n",
    "* Data source heterogeneity is critical for improving the downstream performance\n",
    "\n",
    "* Data curriculum: order in which each data source is scheduled for training\n",
    "\n",
    "* Adaptive adjustment of data proportions for different sources during pre-training (easy samples first, then progressively introducing more challenging/specialized ones)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "Transformer is the common architecture behind PLMs\n",
    "\n",
    "#### Encoder-decoder\n",
    "\n",
    "Vanilla Transformer model with encoder and decoder stacks\n",
    "\n",
    "Mainly for text generation\n",
    "\n",
    "Examples: T5, BART, NLLB\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "Training based on masked tokens\n",
    "\n",
    "Mainly for classification task as document classification or sentiment analysis.\n",
    "\n",
    "Examples: BERT, ViT, etc.\n",
    "\n",
    "#### Causal decoder\n",
    "\n",
    "Unidirectional attention mask to guarantee that each input token can only attend to the past tokens and itself.\n",
    "\n",
    "Mainly for text generation\n",
    "\n",
    "Examples: GPT, LlaMa and Gemma families\n",
    "\n",
    "#### Prefix (Non-casual) decoder\n",
    "\n",
    "Masking mechanism of causal decoders, to enable performing bidirectional attention over the prefix tokens and unidi-\n",
    "rectional attention only on generated tokens\n",
    "\n",
    "Examples: GLM-130B and U-PaLM\n",
    "\n",
    "#### Mixture-of-Experts\n",
    "\n",
    "A subset of neural network weights for each input are sparsely activated\n",
    "\n",
    "Examples: Switch Transformer and GLaM\n",
    "\n",
    "#### Emergent architectures\n",
    "\n",
    "Most based on parameterized state space models (SSM) \n",
    "\n",
    "Examples: Mamba, RetNet, RWKV and Hyena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "Detailed configuration: normalization, position embeddings, activation functions, and attention and bias\n",
    "\n",
    "#### Configuration \n",
    "\n",
    "<!- https://www.reddit.com/r/MachineLearning/comments/t5dznr/r_deepnet_scaling_transformers_to_1000_layers/ >\n",
    "* Normalization: LayerNorm (LN), RMSNorm, DeepNorm and Post-LN, Pre-LN, Sandwich-LN\n",
    "\n",
    "<!- GLU paper: https://arxiv.org/pdf/1612.08083v3>\n",
    "* Activation functions: GeLU, variants of GLU (SwiGLEU, GeGLU)\n",
    "\n",
    "* Position Embeddings: Absolute, relative, rotatory (RoPE), ALiBi\n",
    "\n",
    "* Attention: Full (Vanilla Transformer), Sparse (local based on position), multi-query (key and value matrices shared across heads), grouped query (key and value matrices shared across group of heads), FlashAttention (optimization of GPU memory usage)\n",
    "\n",
    "### Objective functions\n",
    "\n",
    "#### Language modeling (LM)\n",
    "\n",
    "Given a sequence of tokens $x_1^J$, a general training objective is to maximize the following log-likelihood:\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\cal L}(x) &= \\sum_{j=1}^J \\log p(x_i\\mid x_1^{j-1})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Denoising Autoencoding (DAE)\n",
    "\n",
    "Given a sequence of tokens $x$ in which a subset of them $\\tilde{X}$ have been corrupted in order to generate $\\tilde{x}$, a corrupted version of $x$, the objective function maximises the log-likelihood of the corrupted tokens:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\cal L}(x) &= \\sum_{u \\in \\tilde{X}} \\log p(u \\mid \\tilde{x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Mixture-of-Denoisers\n",
    "\n",
    "The loss function is computed according to the type of denoising that is applied to the sample. \n",
    "Samples are prefixed with the denoising type. \n",
    "Denoising varies in length (number of tokens involved) and ratio of corrupted text to generate:\n",
    "\n",
    "* S-denoiser (LM)\n",
    "* R-denoiser (DAE, short span and low corruption)\n",
    "* X-denoiser (DAE, long span or high corruption)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding strategies\n",
    "\n",
    "Greedy and beam search with length penalty\n",
    "\n",
    "$$\n",
    "\\hat{y}_1^{\\hat{I }} &= \\argmax_{y_1^I} P(y_1^I\\mid x_1^J)\n",
    "$$\n",
    "\n",
    "Random sampling: randomly select the next token based on the probability distribution to enhance the randomness and diversity during generation\n",
    "\n",
    "* Top-$k$ sampling: Randomly sample from $$ most probable tokens\n",
    "\n",
    "* Top-$p$ sampling: Randomly sample from those tokens accumulating a probability mass of $p$\n",
    "\n",
    "* Other strategies: $\\eta$-sampling, contrastive search and typical sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of pre-trained large models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"EvolutionaryTreeLLMs.jpg\" width=\"640\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional bibliography\n",
    "\n",
    "<ol>\n",
    "<li><a href=\"https://arxiv.org/pdf/2303.18223\" target=\"_blank\">W. X. Zhao et al. A Survey of Large Language Models, arXiv preprint arXiv:2303.18223 (September 2024).</a></li>\n",
    "<li><a href=\"https://doi.org/10.1145/3649506\" target=\"_blank\">J. Yang et al. Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond, ACM Trans. Knowl. Discov. Data 18, 6, Article 160 (July 2024).</a></li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
