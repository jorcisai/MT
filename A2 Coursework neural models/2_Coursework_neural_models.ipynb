{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2. Coursework on neural models (3.75 points)\n",
    "The goal of this coursework is to apply pre-trained translation models to a translation task selected by the student. To this purpose a series of Jupyter Notebooks as examples have been provided applying NLLB and LLAMA-2 models to the translation task Europarl-ST.\n",
    "\n",
    "### Recommended steps \n",
    "\n",
    "1\\. Select one dataset for a [translation task from HuggingFace](https://huggingface.co/datasets?task_categories=task_categories:translation) and inform the lecturers on your choice to make sure that no other student has selected it\n",
    "\n",
    "2\\. Run experiments from provided notebooks on your selected dataset. Report BLEU and COMET in all cases.\n",
    "\n",
    "3\\. Run experiments on one alternative pre-trained model\n",
    "\n",
    "4\\. Work on extensions of the previous notebooks \n",
    "\n",
    "  - Run experiments on one additional alternative model\n",
    "\n",
    "  - Explore training parameters\n",
    "\n",
    "  - Explore inference parameters\n",
    "\n",
    "  - Explore an alternative existing PEFT method\n",
    "\n",
    "  - Report on additional evaluation measures\n",
    "  \n",
    "  - Extension proposed by the student\n",
    "\n",
    "  ### Submission (deadline: 28/01/2025)\n",
    "\n",
    "  1\\. This coursework is intended to be carried out individually\n",
    "\n",
    "  2\\. A report of maximum 4 pages must be submitted including:\n",
    "\n",
    "  - Brief description of the selected dataset and limitations adopted to run the experiments on your dataset according to your computing capabilities (maximum number of tokens, number of samples in training, validation and test, etc.) \n",
    "\n",
    "  - BLEU and COMET scores for provided notebooks (baseline NLLB, LoRA-finetunned NLLB, 1-shot LLAMA-2, LoRA-finetunned LLAMA-2)\n",
    "\n",
    "  - BLEU and COMET scores on one alternative pre-trained model (baseline and LoRA-finetunned)\n",
    "\n",
    "  - BLEU and COMET scores on your own extensions\n",
    "\n",
    "  - Conclusions\n",
    "\n",
    "  3\\. Notebooks:\n",
    "\n",
    "  - Those four to run baseline NLLB, LoRA-finetunned NLLB, 1-shot LLAMA-2 and LoRA-finetunned LLAMA-2 experiments\n",
    "\n",
    "  - Those two to run baseline and LoRA-finetunned experiments with the pre-trained model of your preference\n",
    "\n",
    "  - Those you need to run your own extensions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
